{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from relu_net import NeuralNet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relu_net.NeuralNet"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NeuralNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from mnist import MNIST"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-5-bed37458e043>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-5-bed37458e043>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    iters = 500\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "alpha = 1\n",
    "    iters = 500\n",
    "\n",
    "    mndata = MNIST('./')\n",
    "    images, labels = mndata.load_training()\n",
    "    tImages, tLabels = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "alpha = 1\n",
    "iters = 500\n",
    "\n",
    "mndata = MNIST('./')\n",
    "images, labels = mndata.load_training()\n",
    "tImages, tLabels = mndata.load_testing()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "images = images[:6000]\n",
    "labels = labels[:6000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fe508c02a94d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtoBinary\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvectorize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mx\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0;36m0.5\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mnormalize_images\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0marr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "toBinary = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "def normalize_images(images):\n",
    "    arr = []\n",
    "    for x in range(len(images)):\n",
    "        arr.append(np.array(images[x]).T.dot(1.0/255))\n",
    "    return np.array(arr).T\n",
    "\n",
    "def normalize_labels(labels):\n",
    "    newLabels = []\n",
    "    for label in labels:\n",
    "        if label == 5:\n",
    "            newLabels.append([1])\n",
    "        else:\n",
    "            newLabels.append([0])\n",
    "    return np.array(newLabels).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-314adb301535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'np'"
     ]
    }
   ],
   "source": [
    "from numpy import np\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'np'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-314adb301535>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'np'"
     ]
    }
   ],
   "source": [
    "from numpy import np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toBinary = np.vectorize(lambda x: 1 if x >= 0.5 else 0)\n",
    "\n",
    "def normalize_images(images):\n",
    "    arr = []\n",
    "    for x in range(len(images)):\n",
    "        arr.append(np.array(images[x]).T.dot(1.0/255))\n",
    "    return np.array(arr).T\n",
    "\n",
    "def normalize_labels(labels):\n",
    "    newLabels = []\n",
    "    for label in labels:\n",
    "        if label == 5:\n",
    "            newLabels.append([1])\n",
    "        else:\n",
    "            newLabels.append([0])\n",
    "    return np.array(newLabels).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "    images = normalize_images(images)\n",
    "    labels = normalize_labels(labels)\n",
    "    tImages = normalize_images(tImages)\n",
    "    tLabels = normalize_labels(tLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'net' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-28f0dba0a232>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m net.train(images, labels, iters, alpha, silent=False, verbose=True,\n\u001b[0m\u001b[1;32m      2\u001b[0m        errors=True, tX=tImages, tY=tLabels)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'net' is not defined"
     ]
    }
   ],
   "source": [
    " net.train(images, labels, iters, alpha, silent=False, verbose=True,\n",
    "        errors=True, tX=tImages, tY=tLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "net = NeuralNet.build([784,5,5,1],\"relu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 500\n",
      "Cost is 0.694121950007\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 1 of 500\n",
      "Cost is 0.542600599023\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 2 of 500\n",
      "Cost is 0.456591457655\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 3 of 500\n",
      "Cost is 0.405051108002\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 4 of 500\n",
      "Cost is 0.372525487087\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 5 of 500\n",
      "Cost is 0.351034376652\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 6 of 500\n",
      "Cost is 0.336283338529\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 7 of 500\n",
      "Cost is 0.325839764177\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 8 of 500\n",
      "Cost is 0.318256542189\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 9 of 500\n",
      "Cost is 0.312634692215\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 10 of 500\n",
      "Cost is 0.308394464463\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 11 of 500\n",
      "Cost is 0.305149798262\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 12 of 500\n",
      "Cost is 0.302636442714\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 13 of 500\n",
      "Cost is 0.300669187764\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 14 of 500\n",
      "Cost is 0.299115533602\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 15 of 500\n",
      "Cost is 0.297878983898\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 16 of 500\n",
      "Cost is 0.296888158317\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 17 of 500\n",
      "Cost is 0.29608952691\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 18 of 500\n",
      "Cost is 0.295442457809\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 19 of 500\n",
      "Cost is 0.294915777139\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 20 of 500\n",
      "Cost is 0.294485338403\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 21 of 500\n",
      "Cost is 0.294132278554\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 22 of 500\n",
      "Cost is 0.293841749313\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 23 of 500\n",
      "Cost is 0.293601982524\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 24 of 500\n",
      "Cost is 0.293403593689\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 25 of 500\n",
      "Cost is 0.293239057549\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 26 of 500\n",
      "Cost is 0.293102309426\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 27 of 500\n",
      "Cost is 0.29298843953\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 28 of 500\n",
      "Cost is 0.292893456662\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 29 of 500\n",
      "Cost is 0.292814104218\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 30 of 500\n",
      "Cost is 0.292747715953\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 31 of 500\n",
      "Cost is 0.292692102211\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 32 of 500\n",
      "Cost is 0.292645459665\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 33 of 500\n",
      "Cost is 0.292606299339\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 34 of 500\n",
      "Cost is 0.292573388926\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 35 of 500\n",
      "Cost is 0.292545706362\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 36 of 500\n",
      "Cost is 0.29252240229\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 37 of 500\n",
      "Cost is 0.292502769614\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 38 of 500\n",
      "Cost is 0.292486218706\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 39 of 500\n",
      "Cost is 0.292472257149\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 40 of 500\n",
      "Cost is 0.292460473148\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 41 of 500\n",
      "Cost is 0.292450521891\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 42 of 500\n",
      "Cost is 0.292442114322\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 43 of 500\n",
      "Cost is 0.292435007865\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 44 of 500\n",
      "Cost is 0.292428998753\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 45 of 500\n",
      "Cost is 0.292423915666\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 46 of 500\n",
      "Cost is 0.292419614445\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 47 of 500\n",
      "Cost is 0.292415973695\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 48 of 500\n",
      "Cost is 0.292412891117\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 49 of 500\n",
      "Cost is 0.292410280451\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 50 of 500\n",
      "Cost is 0.292408068921\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 51 of 500\n",
      "Cost is 0.292406195087\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 52 of 500\n",
      "Cost is 0.292404607061\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 53 of 500\n",
      "Cost is 0.292403260997\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 54 of 500\n",
      "Cost is 0.292402119832\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 55 of 500\n",
      "Cost is 0.292401152223\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 56 of 500\n",
      "Cost is 0.292400331653\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 57 of 500\n",
      "Cost is 0.292399635687\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 58 of 500\n",
      "Cost is 0.292399045328\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 59 of 500\n",
      "Cost is 0.292398544497\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 60 of 500\n",
      "Cost is 0.29239811957\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 61 of 500\n",
      "Cost is 0.29239775901\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 62 of 500\n",
      "Cost is 0.292397453039\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 63 of 500\n",
      "Cost is 0.292397193372\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 64 of 500\n",
      "Cost is 0.292396972983\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 65 of 500\n",
      "Cost is 0.29239678592\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 66 of 500\n",
      "Cost is 0.292396627132\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 67 of 500\n",
      "Cost is 0.292396492338\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 68 of 500\n",
      "Cost is 0.292396377906\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 69 of 500\n",
      "Cost is 0.292396280755\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 70 of 500\n",
      "Cost is 0.292396198272\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 71 of 500\n",
      "Cost is 0.292396128239\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 72 of 500\n",
      "Cost is 0.292396068774\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 73 of 500\n",
      "Cost is 0.292396018282\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 74 of 500\n",
      "Cost is 0.292395975406\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 75 of 500\n",
      "Cost is 0.292395938997\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 76 of 500\n",
      "Cost is 0.292395908078\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 77 of 500\n",
      "Cost is 0.292395881822\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 78 of 500\n",
      "Cost is 0.292395859523\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 79 of 500\n",
      "Cost is 0.292395840586\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 80 of 500\n",
      "Cost is 0.292395824504\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 81 of 500\n",
      "Cost is 0.292395810845\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 82 of 500\n",
      "Cost is 0.292395799244\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 83 of 500\n",
      "Cost is 0.292395789391\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 84 of 500\n",
      "Cost is 0.292395781023\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 85 of 500\n",
      "Cost is 0.292395773915\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 86 of 500\n",
      "Cost is 0.292395767879\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 87 of 500\n",
      "Cost is 0.292395762751\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 88 of 500\n",
      "Cost is 0.292395758396\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 89 of 500\n",
      "Cost is 0.292395754697\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 90 of 500\n",
      "Cost is 0.292395751555\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 91 of 500\n",
      "Cost is 0.292395748886\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 92 of 500\n",
      "Cost is 0.292395746619\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 93 of 500\n",
      "Cost is 0.292395744694\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 94 of 500\n",
      "Cost is 0.292395743058\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 95 of 500\n",
      "Cost is 0.292395741669\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 96 of 500\n",
      "Cost is 0.292395740489\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 97 of 500\n",
      "Cost is 0.292395739486\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 98 of 500\n",
      "Cost is 0.292395738635\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 99 of 500\n",
      "Cost is 0.292395737911\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 100 of 500\n",
      "Cost is 0.292395737297\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 101 of 500\n",
      "Cost is 0.292395736775\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 102 of 500\n",
      "Cost is 0.292395736332\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 103 of 500\n",
      "Cost is 0.292395735955\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 104 of 500\n",
      "Cost is 0.292395735635\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 105 of 500\n",
      "Cost is 0.292395735364\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 106 of 500\n",
      "Cost is 0.292395735133\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 107 of 500\n",
      "Cost is 0.292395734937\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 108 of 500\n",
      "Cost is 0.29239573477\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 109 of 500\n",
      "Cost is 0.292395734629\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 110 of 500\n",
      "Cost is 0.292395734508\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 111 of 500\n",
      "Cost is 0.292395734406\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 112 of 500\n",
      "Cost is 0.29239573432\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 113 of 500\n",
      "Cost is 0.292395734246\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 114 of 500\n",
      "Cost is 0.292395734183\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 115 of 500\n",
      "Cost is 0.29239573413\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 116 of 500\n",
      "Cost is 0.292395734085\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 117 of 500\n",
      "Cost is 0.292395734047\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 118 of 500\n",
      "Cost is 0.292395734014\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 119 of 500\n",
      "Cost is 0.292395733986\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 120 of 500\n",
      "Cost is 0.292395733963\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 121 of 500\n",
      "Cost is 0.292395733943\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 122 of 500\n",
      "Cost is 0.292395733926\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 123 of 500\n",
      "Cost is 0.292395733912\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 124 of 500\n",
      "Cost is 0.292395733899\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 125 of 500\n",
      "Cost is 0.292395733889\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 126 of 500\n",
      "Cost is 0.29239573388\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 127 of 500\n",
      "Cost is 0.292395733873\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 128 of 500\n",
      "Cost is 0.292395733866\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 129 of 500\n",
      "Cost is 0.292395733861\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 130 of 500\n",
      "Cost is 0.292395733856\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 131 of 500\n",
      "Cost is 0.292395733852\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 132 of 500\n",
      "Cost is 0.292395733849\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 133 of 500\n",
      "Cost is 0.292395733846\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 134 of 500\n",
      "Cost is 0.292395733844\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 135 of 500\n",
      "Cost is 0.292395733842\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 136 of 500\n",
      "Cost is 0.29239573384\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 137 of 500\n",
      "Cost is 0.292395733839\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 138 of 500\n",
      "Cost is 0.292395733837\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 139 of 500\n",
      "Cost is 0.292395733836\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 140 of 500\n",
      "Cost is 0.292395733835\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 141 of 500\n",
      "Cost is 0.292395733835\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 142 of 500\n",
      "Cost is 0.292395733834\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 143 of 500\n",
      "Cost is 0.292395733833\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 144 of 500\n",
      "Cost is 0.292395733833\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 145 of 500\n",
      "Cost is 0.292395733833\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 146 of 500\n",
      "Cost is 0.292395733832\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 147 of 500\n",
      "Cost is 0.292395733832\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 148 of 500\n",
      "Cost is 0.292395733832\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 149 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 150 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 151 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 152 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 153 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 154 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 155 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 156 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 157 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 158 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 159 of 500\n",
      "Cost is 0.292395733831\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 160 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 161 of 500\n",
      "Cost is 0.29239573383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 162 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 163 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 164 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 165 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 166 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 167 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 168 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 169 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 170 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 171 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 172 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 173 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 174 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 175 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 176 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 177 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 178 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 179 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 180 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 181 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 182 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 183 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 184 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 185 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 186 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 187 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 188 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 189 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 190 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 191 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 192 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 193 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 194 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 195 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 196 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 197 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 198 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 199 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 200 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 201 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 202 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 203 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 204 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 205 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 206 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 207 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 208 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 209 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 210 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 211 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 212 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 213 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 214 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 215 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 216 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 217 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 218 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 219 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 220 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 221 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 222 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 223 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 224 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 225 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 226 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 227 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 228 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 229 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 230 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 231 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 232 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 233 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 234 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 235 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 236 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 237 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 238 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 239 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 240 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 241 of 500\n",
      "Cost is 0.29239573383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 242 of 500\n",
      "Cost is 0.29239573383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 243 of 500\n",
      "Cost is 0.292394885282\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 244 of 500\n",
      "Cost is 0.292394885099\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 245 of 500\n",
      "Cost is 0.292394884916\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 246 of 500\n",
      "Cost is 0.292394884734\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 247 of 500\n",
      "Cost is 0.292394884551\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 248 of 500\n",
      "Cost is 0.292394884369\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 249 of 500\n",
      "Cost is 0.292394884186\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 250 of 500\n",
      "Cost is 0.292394884004\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 251 of 500\n",
      "Cost is 0.292394883821\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 252 of 500\n",
      "Cost is 0.292394883639\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 253 of 500\n",
      "Cost is 0.292394033082\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 254 of 500\n",
      "Cost is 0.292394032352\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 255 of 500\n",
      "Cost is 0.292394031622\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 256 of 500\n",
      "Cost is 0.292394030892\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 257 of 500\n",
      "Cost is 0.292394030163\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 258 of 500\n",
      "Cost is 0.292394029433\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 259 of 500\n",
      "Cost is 0.292394028704\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 260 of 500\n",
      "Cost is 0.292393175045\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 261 of 500\n",
      "Cost is 0.292392319926\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 262 of 500\n",
      "Cost is 0.292392317008\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 263 of 500\n",
      "Cost is 0.29239231409\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 264 of 500\n",
      "Cost is 0.292390599838\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 265 of 500\n",
      "Cost is 0.292390593274\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 266 of 500\n",
      "Cost is 0.292389728854\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 267 of 500\n",
      "Cost is 0.292389719924\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 268 of 500\n",
      "Cost is 0.292389710996\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 269 of 500\n",
      "Cost is 0.292389702071\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 270 of 500\n",
      "Cost is 0.292387967224\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 271 of 500\n",
      "Cost is 0.292385358666\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 272 of 500\n",
      "Cost is 0.292385332442\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 273 of 500\n",
      "Cost is 0.292383568279\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 274 of 500\n",
      "Cost is 0.292382661084\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 275 of 500\n",
      "Cost is 0.292379997384\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 276 of 500\n",
      "Cost is 0.29237730586\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 277 of 500\n",
      "Cost is 0.292373700272\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 278 of 500\n",
      "Cost is 0.292371814899\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 279 of 500\n",
      "Cost is 0.29236633767\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 280 of 500\n",
      "Cost is 0.292362552814\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 281 of 500\n",
      "Cost is 0.292355979926\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 282 of 500\n",
      "Cost is 0.292351071645\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 283 of 500\n",
      "Cost is 0.292340512948\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 284 of 500\n",
      "Cost is 0.292333343432\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 285 of 500\n",
      "Cost is 0.292324042942\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 286 of 500\n",
      "Cost is 0.292318216577\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 287 of 500\n",
      "Cost is 0.292311206378\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 288 of 500\n",
      "Cost is 0.292302936425\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 289 of 500\n",
      "Cost is 0.292291311859\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 290 of 500\n",
      "Cost is 0.292278127909\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 291 of 500\n",
      "Cost is 0.292265336453\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 292 of 500\n",
      "Cost is 0.292249770818\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 293 of 500\n",
      "Cost is 0.292233363782\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 294 of 500\n",
      "Cost is 0.292209346623\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 295 of 500\n",
      "Cost is 0.292184956069\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 296 of 500\n",
      "Cost is 0.292162521477\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 297 of 500\n",
      "Cost is 0.292131415909\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 298 of 500\n",
      "Cost is 0.292093130864\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 299 of 500\n",
      "Cost is 0.292039040627\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 300 of 500\n",
      "Cost is 0.291996436247\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 301 of 500\n",
      "Cost is 0.291896361673\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 302 of 500\n",
      "Cost is 0.291825056043\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 303 of 500\n",
      "Cost is 0.291721523451\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 304 of 500\n",
      "Cost is 0.291610455524\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 305 of 500\n",
      "Cost is 0.291462939058\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 306 of 500\n",
      "Cost is 0.291263598429\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 307 of 500\n",
      "Cost is 0.291004409643\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 308 of 500\n",
      "Cost is 0.290712727221\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 309 of 500\n",
      "Cost is 0.290305010046\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 310 of 500\n",
      "Cost is 0.289871224419\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 311 of 500\n",
      "Cost is 0.289302900494\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 312 of 500\n",
      "Cost is 0.28857892214\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 313 of 500\n",
      "Cost is 0.287748311594\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 314 of 500\n",
      "Cost is 0.286623727487\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 315 of 500\n",
      "Cost is 0.285251823742\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 316 of 500\n",
      "Cost is 0.283684047062\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 317 of 500\n",
      "Cost is 0.281828710049\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 318 of 500\n",
      "Cost is 0.279546583277\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 319 of 500\n",
      "Cost is 0.276919721525\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 320 of 500\n",
      "Cost is 0.274200274166\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 321 of 500\n",
      "Cost is 0.271000621193\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 322 of 500\n",
      "Cost is 0.267645350453\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 323 of 500\n",
      "Cost is 0.264388731012\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 324 of 500\n",
      "Cost is 0.260677826383\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 325 of 500\n",
      "Cost is 0.256825067166\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 326 of 500\n",
      "Cost is 0.252351739409\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 327 of 500\n",
      "Cost is 0.248557996273\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 328 of 500\n",
      "Cost is 0.244662478789\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 329 of 500\n",
      "Cost is 0.240906981191\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 330 of 500\n",
      "Cost is 0.237362574948\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 331 of 500\n",
      "Cost is 0.23312769838\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 332 of 500\n",
      "Cost is 0.229456835069\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 333 of 500\n",
      "Cost is 0.225317682358\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 334 of 500\n",
      "Cost is 0.222100365199\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 335 of 500\n",
      "Cost is 0.217577700949\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 336 of 500\n",
      "Cost is 0.215578033574\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 337 of 500\n",
      "Cost is 0.209786494772\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 338 of 500\n",
      "Cost is 0.213262042797\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 339 of 500\n",
      "Cost is 0.20487204339\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 340 of 500\n",
      "Cost is 0.218307611025\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 341 of 500\n",
      "Cost is 0.205515020647\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 342 of 500\n",
      "Cost is 0.226460492494\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 343 of 500\n",
      "Cost is 0.201199936674\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 344 of 500\n",
      "Cost is 0.214994819577\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 345 of 500\n",
      "Cost is 0.195007474484\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 346 of 500\n",
      "Cost is 0.203776108343\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 347 of 500\n",
      "Cost is 0.189042999246\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 348 of 500\n",
      "Cost is 0.1988595082\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 349 of 500\n",
      "Cost is 0.185438834929\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 350 of 500\n",
      "Cost is 0.194903312699\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 351 of 500\n",
      "Cost is 0.182199037852\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 352 of 500\n",
      "Cost is 0.190858885081\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 353 of 500\n",
      "Cost is 0.17817035979\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 354 of 500\n",
      "Cost is 0.187159445987\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 355 of 500\n",
      "Cost is 0.173816660707\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 356 of 500\n",
      "Cost is 0.182333884251\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 357 of 500\n",
      "Cost is 0.170203223137\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 358 of 500\n",
      "Cost is 0.178816101883\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 359 of 500\n",
      "Cost is 0.167313886777\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 360 of 500\n",
      "Cost is 0.1739987698\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 361 of 500\n",
      "Cost is 0.162679314169\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 362 of 500\n",
      "Cost is 0.170519699946\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 363 of 500\n",
      "Cost is 0.160311344888\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 364 of 500\n",
      "Cost is 0.16852908079\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 365 of 500\n",
      "Cost is 0.158057490616\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 366 of 500\n",
      "Cost is 0.166333283528\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 367 of 500\n",
      "Cost is 0.155540313852\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 368 of 500\n",
      "Cost is 0.164654592207\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 369 of 500\n",
      "Cost is 0.15328192866\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 370 of 500\n",
      "Cost is 0.159323042271\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 371 of 500\n",
      "Cost is 0.147650202208\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 372 of 500\n",
      "Cost is 0.149760592206\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 373 of 500\n",
      "Cost is 0.139408440038\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 374 of 500\n",
      "Cost is 0.139514518225\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 375 of 500\n",
      "Cost is 0.132338432442\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 376 of 500\n",
      "Cost is 0.131250571783\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 377 of 500\n",
      "Cost is 0.127194426662\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 378 of 500\n",
      "Cost is 0.126851756589\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 379 of 500\n",
      "Cost is 0.123370049292\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 380 of 500\n",
      "Cost is 0.122954640376\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 381 of 500\n",
      "Cost is 0.119674266495\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 382 of 500\n",
      "Cost is 0.120061637628\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 383 of 500\n",
      "Cost is 0.118353194549\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 384 of 500\n",
      "Cost is 0.119832938283\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 385 of 500\n",
      "Cost is 0.11755700757\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 386 of 500\n",
      "Cost is 0.124409226234\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 387 of 500\n",
      "Cost is 0.12718136158\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 388 of 500\n",
      "Cost is 0.148489252194\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 389 of 500\n",
      "Cost is 0.178928112282\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 390 of 500\n",
      "Cost is 0.200034192574\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 391 of 500\n",
      "Cost is 0.227602848554\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 392 of 500\n",
      "Cost is 0.203110739179\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 393 of 500\n",
      "Cost is 0.16497311992\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 394 of 500\n",
      "Cost is 0.132538053118\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 395 of 500\n",
      "Cost is 0.112831028978\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 396 of 500\n",
      "Cost is 0.107041333401\n",
      "Training correct is: 0.914333333333\n",
      "Test correct is: 0.9108\n",
      "Iteration 397 of 500\n",
      "Cost is 0.10225388858\n",
      "Training correct is: 0.916166666667\n",
      "Test correct is: 0.9136\n",
      "Iteration 398 of 500\n",
      "Cost is 0.102245700695\n",
      "Training correct is: 0.976833333333\n",
      "Test correct is: 0.9679\n",
      "Iteration 399 of 500\n",
      "Cost is 0.099017985026\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct is: 0.9755\n",
      "Test correct is: 0.9664\n",
      "Iteration 400 of 500\n",
      "Cost is 0.0990920589005\n",
      "Training correct is: 0.976666666667\n",
      "Test correct is: 0.9681\n",
      "Iteration 401 of 500\n",
      "Cost is 0.0969416380514\n",
      "Training correct is: 0.9765\n",
      "Test correct is: 0.9677\n",
      "Iteration 402 of 500\n",
      "Cost is 0.0961188743195\n",
      "Training correct is: 0.977666666667\n",
      "Test correct is: 0.9688\n",
      "Iteration 403 of 500\n",
      "Cost is 0.0946754533871\n",
      "Training correct is: 0.977833333333\n",
      "Test correct is: 0.9693\n",
      "Iteration 404 of 500\n",
      "Cost is 0.0941157362506\n",
      "Training correct is: 0.978833333333\n",
      "Test correct is: 0.9704\n",
      "Iteration 405 of 500\n",
      "Cost is 0.0923116116897\n",
      "Training correct is: 0.978833333333\n",
      "Test correct is: 0.9714\n",
      "Iteration 406 of 500\n",
      "Cost is 0.0916428725487\n",
      "Training correct is: 0.979666666667\n",
      "Test correct is: 0.9703\n",
      "Iteration 407 of 500\n",
      "Cost is 0.0897513923134\n",
      "Training correct is: 0.9795\n",
      "Test correct is: 0.9721\n",
      "Iteration 408 of 500\n",
      "Cost is 0.0889266019572\n",
      "Training correct is: 0.98\n",
      "Test correct is: 0.9712\n",
      "Iteration 409 of 500\n",
      "Cost is 0.0877901655711\n",
      "Training correct is: 0.979666666667\n",
      "Test correct is: 0.9726\n",
      "Iteration 410 of 500\n",
      "Cost is 0.0870382558884\n",
      "Training correct is: 0.980833333333\n",
      "Test correct is: 0.9719\n",
      "Iteration 411 of 500\n",
      "Cost is 0.0852739508567\n",
      "Training correct is: 0.980333333333\n",
      "Test correct is: 0.9736\n",
      "Iteration 412 of 500\n",
      "Cost is 0.0851723474275\n",
      "Training correct is: 0.981166666667\n",
      "Test correct is: 0.9723\n",
      "Iteration 413 of 500\n",
      "Cost is 0.0833183321736\n",
      "Training correct is: 0.981\n",
      "Test correct is: 0.974\n",
      "Iteration 414 of 500\n",
      "Cost is 0.0832578613996\n",
      "Training correct is: 0.981666666667\n",
      "Test correct is: 0.9725\n",
      "Iteration 415 of 500\n",
      "Cost is 0.0812576622415\n",
      "Training correct is: 0.9815\n",
      "Test correct is: 0.974\n",
      "Iteration 416 of 500\n",
      "Cost is 0.0816179744017\n",
      "Training correct is: 0.982\n",
      "Test correct is: 0.9725\n",
      "Iteration 417 of 500\n",
      "Cost is 0.0790821280988\n",
      "Training correct is: 0.981333333333\n",
      "Test correct is: 0.9745\n",
      "Iteration 418 of 500\n",
      "Cost is 0.0809658335401\n",
      "Training correct is: 0.981666666667\n",
      "Test correct is: 0.9719\n",
      "Iteration 419 of 500\n",
      "Cost is 0.0785550630032\n",
      "Training correct is: 0.982166666667\n",
      "Test correct is: 0.9744\n",
      "Iteration 420 of 500\n",
      "Cost is 0.0795434502016\n",
      "Training correct is: 0.9815\n",
      "Test correct is: 0.9711\n",
      "Iteration 421 of 500\n",
      "Cost is 0.0782825000759\n",
      "Training correct is: 0.980833333333\n",
      "Test correct is: 0.9741\n",
      "Iteration 422 of 500\n",
      "Cost is 0.0811625365264\n",
      "Training correct is: 0.979333333333\n",
      "Test correct is: 0.968\n",
      "Iteration 423 of 500\n",
      "Cost is 0.0822037333401\n",
      "Training correct is: 0.976833333333\n",
      "Test correct is: 0.9735\n",
      "Iteration 424 of 500\n",
      "Cost is 0.0870331079295\n",
      "Training correct is: 0.973166666667\n",
      "Test correct is: 0.9613\n",
      "Iteration 425 of 500\n",
      "Cost is 0.0957562252073\n",
      "Training correct is: 0.968833333333\n",
      "Test correct is: 0.9687\n",
      "Iteration 426 of 500\n",
      "Cost is 0.100976149214\n",
      "Training correct is: 0.960333333333\n",
      "Test correct is: 0.9472\n",
      "Iteration 427 of 500\n",
      "Cost is 0.132172246861\n",
      "Training correct is: 0.9375\n",
      "Test correct is: 0.9267\n",
      "Iteration 428 of 500\n",
      "Cost is 0.151954906459\n",
      "Training correct is: 0.936333333333\n",
      "Test correct is: 0.9233\n",
      "Iteration 429 of 500\n",
      "Cost is 0.253921545883\n",
      "Training correct is: 0.921166666667\n",
      "Test correct is: 0.9144\n",
      "Iteration 430 of 500\n",
      "Cost is 0.21075641313\n",
      "Training correct is: 0.936333333333\n",
      "Test correct is: 0.9242\n",
      "Iteration 431 of 500\n",
      "Cost is 0.241168145078\n",
      "Training correct is: 0.955166666667\n",
      "Test correct is: 0.9582\n",
      "Iteration 432 of 500\n",
      "Cost is 0.148762967336\n",
      "Training correct is: 0.963166666667\n",
      "Test correct is: 0.9525\n",
      "Iteration 433 of 500\n",
      "Cost is 0.116503067394\n",
      "Training correct is: 0.982\n",
      "Test correct is: 0.9742\n",
      "Iteration 434 of 500\n",
      "Cost is 0.0854079734742\n",
      "Training correct is: 0.9815\n",
      "Test correct is: 0.973\n",
      "Iteration 435 of 500\n",
      "Cost is 0.0761084604388\n",
      "Training correct is: 0.982833333333\n",
      "Test correct is: 0.9754\n",
      "Iteration 436 of 500\n",
      "Cost is 0.0744658488815\n",
      "Training correct is: 0.982166666667\n",
      "Test correct is: 0.9758\n",
      "Iteration 437 of 500\n",
      "Cost is 0.0738481589097\n",
      "Training correct is: 0.982833333333\n",
      "Test correct is: 0.9753\n",
      "Iteration 438 of 500\n",
      "Cost is 0.0719176550564\n",
      "Training correct is: 0.983166666667\n",
      "Test correct is: 0.9755\n",
      "Iteration 439 of 500\n",
      "Cost is 0.0707778664917\n",
      "Training correct is: 0.983333333333\n",
      "Test correct is: 0.9757\n",
      "Iteration 440 of 500\n",
      "Cost is 0.0701164832587\n",
      "Training correct is: 0.983166666667\n",
      "Test correct is: 0.9758\n",
      "Iteration 441 of 500\n",
      "Cost is 0.0695495361439\n",
      "Training correct is: 0.983333333333\n",
      "Test correct is: 0.9758\n",
      "Iteration 442 of 500\n",
      "Cost is 0.0688320930081\n",
      "Training correct is: 0.983666666667\n",
      "Test correct is: 0.9761\n",
      "Iteration 443 of 500\n",
      "Cost is 0.067919322621\n",
      "Training correct is: 0.984166666667\n",
      "Test correct is: 0.9761\n",
      "Iteration 444 of 500\n",
      "Cost is 0.0665887907153\n",
      "Training correct is: 0.984333333333\n",
      "Test correct is: 0.976\n",
      "Iteration 445 of 500\n",
      "Cost is 0.0663588831932\n",
      "Training correct is: 0.984833333333\n",
      "Test correct is: 0.9763\n",
      "Iteration 446 of 500\n",
      "Cost is 0.0648918780462\n",
      "Training correct is: 0.985166666667\n",
      "Test correct is: 0.9763\n",
      "Iteration 447 of 500\n",
      "Cost is 0.0641765191962\n",
      "Training correct is: 0.984833333333\n",
      "Test correct is: 0.9766\n",
      "Iteration 448 of 500\n",
      "Cost is 0.0641202058556\n",
      "Training correct is: 0.9855\n",
      "Test correct is: 0.9768\n",
      "Iteration 449 of 500\n",
      "Cost is 0.0625003900916\n",
      "Training correct is: 0.985\n",
      "Test correct is: 0.9767\n",
      "Iteration 450 of 500\n",
      "Cost is 0.0624458421968\n",
      "Training correct is: 0.9855\n",
      "Test correct is: 0.9772\n",
      "Iteration 451 of 500\n",
      "Cost is 0.0613750493762\n",
      "Training correct is: 0.985333333333\n",
      "Test correct is: 0.9766\n",
      "Iteration 452 of 500\n",
      "Cost is 0.0611502721508\n",
      "Training correct is: 0.986166666667\n",
      "Test correct is: 0.9776\n",
      "Iteration 453 of 500\n",
      "Cost is 0.060359603278\n",
      "Training correct is: 0.984666666667\n",
      "Test correct is: 0.9766\n",
      "Iteration 454 of 500\n",
      "Cost is 0.0611637130706\n",
      "Training correct is: 0.9865\n",
      "Test correct is: 0.9772\n",
      "Iteration 455 of 500\n",
      "Cost is 0.0596030202441\n",
      "Training correct is: 0.984666666667\n",
      "Test correct is: 0.9763\n",
      "Iteration 456 of 500\n",
      "Cost is 0.0600227020344\n",
      "Training correct is: 0.986666666667\n",
      "Test correct is: 0.9773\n",
      "Iteration 457 of 500\n",
      "Cost is 0.0583230503072\n",
      "Training correct is: 0.985\n",
      "Test correct is: 0.9765\n",
      "Iteration 458 of 500\n",
      "Cost is 0.0588726890489\n",
      "Training correct is: 0.986666666667\n",
      "Test correct is: 0.9774\n",
      "Iteration 459 of 500\n",
      "Cost is 0.0570658661607\n",
      "Training correct is: 0.984833333333\n",
      "Test correct is: 0.9766\n",
      "Iteration 460 of 500\n",
      "Cost is 0.0582193377994\n",
      "Training correct is: 0.986666666667\n",
      "Test correct is: 0.9774\n",
      "Iteration 461 of 500\n",
      "Cost is 0.056040408753\n",
      "Training correct is: 0.985333333333\n",
      "Test correct is: 0.9767\n",
      "Iteration 462 of 500\n",
      "Cost is 0.0563923353685\n",
      "Training correct is: 0.986833333333\n",
      "Test correct is: 0.9776\n",
      "Iteration 463 of 500\n",
      "Cost is 0.0550989392643\n",
      "Training correct is: 0.985666666667\n",
      "Test correct is: 0.9768\n",
      "Iteration 464 of 500\n",
      "Cost is 0.0551306568769\n",
      "Training correct is: 0.987\n",
      "Test correct is: 0.9779\n",
      "Iteration 465 of 500\n",
      "Cost is 0.054244262641\n",
      "Training correct is: 0.985833333333\n",
      "Test correct is: 0.977\n",
      "Iteration 466 of 500\n",
      "Cost is 0.053875606866\n",
      "Training correct is: 0.987\n",
      "Test correct is: 0.9779\n",
      "Iteration 467 of 500\n",
      "Cost is 0.0528941172546\n",
      "Training correct is: 0.985666666667\n",
      "Test correct is: 0.9772\n",
      "Iteration 468 of 500\n",
      "Cost is 0.0526743400923\n",
      "Training correct is: 0.987\n",
      "Test correct is: 0.9779\n",
      "Iteration 469 of 500\n",
      "Cost is 0.0520283200581\n",
      "Training correct is: 0.985666666667\n",
      "Test correct is: 0.9772\n",
      "Iteration 470 of 500\n",
      "Cost is 0.0517940483182\n",
      "Training correct is: 0.986166666667\n",
      "Test correct is: 0.9792\n",
      "Iteration 471 of 500\n",
      "Cost is 0.0522028600039\n",
      "Training correct is: 0.986333333333\n",
      "Test correct is: 0.9766\n",
      "Iteration 472 of 500\n",
      "Cost is 0.0519141817249\n",
      "Training correct is: 0.986333333333\n",
      "Test correct is: 0.9784\n",
      "Iteration 473 of 500\n",
      "Cost is 0.0517528954395\n",
      "Training correct is: 0.986\n",
      "Test correct is: 0.976\n",
      "Iteration 474 of 500\n",
      "Cost is 0.051956892003\n",
      "Training correct is: 0.985333333333\n",
      "Test correct is: 0.9778\n",
      "Iteration 475 of 500\n",
      "Cost is 0.0522667708015\n",
      "Training correct is: 0.9845\n",
      "Test correct is: 0.9748\n",
      "Iteration 476 of 500\n",
      "Cost is 0.0529815438634\n",
      "Training correct is: 0.984166666667\n",
      "Test correct is: 0.9767\n",
      "Iteration 477 of 500\n",
      "Cost is 0.055339530768\n",
      "Training correct is: 0.9815\n",
      "Test correct is: 0.9725\n",
      "Iteration 478 of 500\n",
      "Cost is 0.063147278408\n",
      "Training correct is: 0.9795\n",
      "Test correct is: 0.9741\n",
      "Iteration 479 of 500\n",
      "Cost is 0.0634777245417\n",
      "Training correct is: 0.973333333333\n",
      "Test correct is: 0.9641\n",
      "Iteration 480 of 500\n",
      "Cost is 0.0870156337028\n",
      "Training correct is: 0.968333333333\n",
      "Test correct is: 0.9642\n",
      "Iteration 481 of 500\n",
      "Cost is 0.0889562156676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training correct is: 0.951833333333\n",
      "Test correct is: 0.9442\n",
      "Iteration 482 of 500\n",
      "Cost is 0.171331739029\n",
      "Training correct is: 0.946666666667\n",
      "Test correct is: 0.9375\n",
      "Iteration 483 of 500\n",
      "Cost is 0.157592943104\n",
      "Training correct is: 0.929833333333\n",
      "Test correct is: 0.9234\n",
      "Iteration 484 of 500\n",
      "Cost is 0.302583807259\n",
      "Training correct is: 0.934666666667\n",
      "Test correct is: 0.9572\n",
      "Iteration 485 of 500\n",
      "Cost is 0.245597691164\n",
      "Training correct is: 0.9385\n",
      "Test correct is: 0.9313\n",
      "Iteration 486 of 500\n",
      "Cost is 0.266003172191\n",
      "Training correct is: 0.955\n",
      "Test correct is: 0.9405\n",
      "Iteration 487 of 500\n",
      "Cost is 0.149628379774\n",
      "Training correct is: 0.948833333333\n",
      "Test correct is: 0.9405\n",
      "Iteration 488 of 500\n",
      "Cost is 0.173212710915\n",
      "Training correct is: 0.983\n",
      "Test correct is: 0.9745\n",
      "Iteration 489 of 500\n",
      "Cost is 0.0737464931281\n",
      "Training correct is: 0.9815\n",
      "Test correct is: 0.9737\n",
      "Iteration 490 of 500\n",
      "Cost is 0.0577264314219\n",
      "Training correct is: 0.985333333333\n",
      "Test correct is: 0.9778\n",
      "Iteration 491 of 500\n",
      "Cost is 0.0532009922808\n",
      "Training correct is: 0.986666666667\n",
      "Test correct is: 0.9779\n",
      "Iteration 492 of 500\n",
      "Cost is 0.0500747036345\n",
      "Training correct is: 0.986166666667\n",
      "Test correct is: 0.9785\n",
      "Iteration 493 of 500\n",
      "Cost is 0.0487584788\n",
      "Training correct is: 0.986166666667\n",
      "Test correct is: 0.9787\n",
      "Iteration 494 of 500\n",
      "Cost is 0.0477092844109\n",
      "Training correct is: 0.987333333333\n",
      "Test correct is: 0.979\n",
      "Iteration 495 of 500\n",
      "Cost is 0.0454095687478\n",
      "Training correct is: 0.987333333333\n",
      "Test correct is: 0.9791\n",
      "Iteration 496 of 500\n",
      "Cost is 0.0449505946227\n",
      "Training correct is: 0.987666666667\n",
      "Test correct is: 0.9791\n",
      "Iteration 497 of 500\n",
      "Cost is 0.0440987987313\n",
      "Training correct is: 0.988\n",
      "Test correct is: 0.9798\n",
      "Iteration 498 of 500\n",
      "Cost is 0.0433099867122\n",
      "Training correct is: 0.9885\n",
      "Test correct is: 0.9796\n",
      "Iteration 499 of 500\n",
      "Cost is 0.0420192231491\n",
      "Training correct is: 0.989166666667\n",
      "Test correct is: 0.9798\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.69412195000668342,\n",
       "  0.54260059902338265,\n",
       "  0.45659145765526304,\n",
       "  0.40505110800152183,\n",
       "  0.37252548708712552,\n",
       "  0.35103437665209192,\n",
       "  0.33628333852890674,\n",
       "  0.32583976417694288,\n",
       "  0.31825654218921634,\n",
       "  0.31263469221476831,\n",
       "  0.30839446446284224,\n",
       "  0.3051497982616625,\n",
       "  0.30263644271355161,\n",
       "  0.30066918776394935,\n",
       "  0.29911553360214393,\n",
       "  0.29787898389826661,\n",
       "  0.29688815831711479,\n",
       "  0.29608952691034512,\n",
       "  0.29544245780855011,\n",
       "  0.29491577713944966,\n",
       "  0.29448533840312557,\n",
       "  0.29413227855426616,\n",
       "  0.29384174931302998,\n",
       "  0.29360198252380282,\n",
       "  0.29340359368925656,\n",
       "  0.2932390575488304,\n",
       "  0.29310230942592175,\n",
       "  0.2929884395303024,\n",
       "  0.29289345666206917,\n",
       "  0.29281410421765475,\n",
       "  0.29274771595314475,\n",
       "  0.29269210221137859,\n",
       "  0.2926454596651975,\n",
       "  0.29260629933875792,\n",
       "  0.29257338892643453,\n",
       "  0.29254570636210414,\n",
       "  0.2925224022899095,\n",
       "  0.29250276961413524,\n",
       "  0.29248621870578517,\n",
       "  0.29247225714929981,\n",
       "  0.29246047314829327,\n",
       "  0.29245052189148552,\n",
       "  0.29244211432205325,\n",
       "  0.29243500786477888,\n",
       "  0.29242899875295741,\n",
       "  0.29242391566622011,\n",
       "  0.29241961444545894,\n",
       "  0.29241597369492889,\n",
       "  0.29241289111676039,\n",
       "  0.29241028045143108,\n",
       "  0.29240806892053423,\n",
       "  0.29240619508670518,\n",
       "  0.29240460706058463,\n",
       "  0.29240326099692832,\n",
       "  0.2924021198319936,\n",
       "  0.29240115222252577,\n",
       "  0.29240033165341289,\n",
       "  0.29239963568659649,\n",
       "  0.29239904532844246,\n",
       "  0.2923985444965318,\n",
       "  0.29239811956996048,\n",
       "  0.29239775900987736,\n",
       "  0.29239745303910625,\n",
       "  0.29239719337152853,\n",
       "  0.292396972983404,\n",
       "  0.29239678592003832,\n",
       "  0.29239662713227976,\n",
       "  0.29239649233819953,\n",
       "  0.29239637790603124,\n",
       "  0.29239628075507001,\n",
       "  0.29239619827177127,\n",
       "  0.29239612823867694,\n",
       "  0.29239606877420765,\n",
       "  0.29239601828164807,\n",
       "  0.2923959754059135,\n",
       "  0.29239593899689087,\n",
       "  0.29239590807837323,\n",
       "  0.29239588182171017,\n",
       "  0.29239585952346048,\n",
       "  0.29239584058644408,\n",
       "  0.29239582450366386,\n",
       "  0.29239581084466709,\n",
       "  0.29239579924397363,\n",
       "  0.29239578939125754,\n",
       "  0.292395781023014,\n",
       "  0.29239577391548616,\n",
       "  0.29239576787866806,\n",
       "  0.29239576275120316,\n",
       "  0.29239575839606619,\n",
       "  0.29239575469688867,\n",
       "  0.29239575155484349,\n",
       "  0.2923957488859994,\n",
       "  0.29239574661907408,\n",
       "  0.29239574469352592,\n",
       "  0.29239574305793581,\n",
       "  0.29239574166863325,\n",
       "  0.29239574048852551,\n",
       "  0.29239573948610709,\n",
       "  0.29239573863461965,\n",
       "  0.29239573791133583,\n",
       "  0.29239573729694934,\n",
       "  0.2923957367750627,\n",
       "  0.2923957363317492,\n",
       "  0.29239573595517654,\n",
       "  0.29239573563529581,\n",
       "  0.2923957353635725,\n",
       "  0.29239573513275585,\n",
       "  0.29239573493668775,\n",
       "  0.29239573477013542,\n",
       "  0.29239573462865531,\n",
       "  0.29239573450847389,\n",
       "  0.29239573440638406,\n",
       "  0.2923957343196622,\n",
       "  0.29239573424599463,\n",
       "  0.29239573418341691,\n",
       "  0.29239573413025927,\n",
       "  0.29239573408510344,\n",
       "  0.29239573404674513,\n",
       "  0.2923957340141598,\n",
       "  0.292395733986481,\n",
       "  0.29239573396296736,\n",
       "  0.29239573394299423,\n",
       "  0.29239573392602641,\n",
       "  0.29239573391161372,\n",
       "  0.29239573389937012,\n",
       "  0.29239573388896944,\n",
       "  0.29239573388013401,\n",
       "  0.29239573387262963,\n",
       "  0.29239573386625428,\n",
       "  0.29239573386083839,\n",
       "  0.29239573385623752,\n",
       "  0.29239573385233009,\n",
       "  0.2923957338490093,\n",
       "  0.29239573384618989,\n",
       "  0.29239573384379469,\n",
       "  0.29239573384175932,\n",
       "  0.29239573384003065,\n",
       "  0.2923957338385621,\n",
       "  0.2923957338373146,\n",
       "  0.2923957338362545,\n",
       "  0.29239573383535494,\n",
       "  0.29239573383459022,\n",
       "  0.29239573383394013,\n",
       "  0.2923957338333889,\n",
       "  0.29239573383291945,\n",
       "  0.29239573383252127,\n",
       "  0.2923957338321837,\n",
       "  0.29239573383189604,\n",
       "  0.29239573383165157,\n",
       "  0.2923957338314449,\n",
       "  0.29239573383126849,\n",
       "  0.29239573383111861,\n",
       "  0.29239573383099193,\n",
       "  0.29239573383088402,\n",
       "  0.29239573383079165,\n",
       "  0.29239573383071388,\n",
       "  0.29239573383064782,\n",
       "  0.29239573383059131,\n",
       "  0.29239573383054412,\n",
       "  0.29239573383050349,\n",
       "  0.29239573383046863,\n",
       "  0.29239573383043926,\n",
       "  0.29239573383041478,\n",
       "  0.29239573383039352,\n",
       "  0.29239573383037581,\n",
       "  0.29239573383035994,\n",
       "  0.29239573383034784,\n",
       "  0.29239573383033673,\n",
       "  0.29239573383032663,\n",
       "  0.29239573383031936,\n",
       "  0.29239573383031281,\n",
       "  0.29239573383030687,\n",
       "  0.29239573383030187,\n",
       "  0.29239573383029727,\n",
       "  0.29239573383029382,\n",
       "  0.29239573383029155,\n",
       "  0.292395733830289,\n",
       "  0.29239573383028689,\n",
       "  0.29239573383028433,\n",
       "  0.29239573383028339,\n",
       "  0.292395733830282,\n",
       "  0.29239573383028072,\n",
       "  0.29239573383028028,\n",
       "  0.29239573383027923,\n",
       "  0.29239573383027867,\n",
       "  0.29239573383027717,\n",
       "  0.29239573383027706,\n",
       "  0.29239573383027684,\n",
       "  0.29239573383027678,\n",
       "  0.29239573383027584,\n",
       "  0.29239573383027545,\n",
       "  0.29239573383027589,\n",
       "  0.29239573383027517,\n",
       "  0.29239573383027539,\n",
       "  0.29239573383027517,\n",
       "  0.29239573383027495,\n",
       "  0.29239573383027506,\n",
       "  0.29239573383027434,\n",
       "  0.29239573383027434,\n",
       "  0.29239573383027434,\n",
       "  0.29239573383027445,\n",
       "  0.29239573383027478,\n",
       "  0.29239573383027423,\n",
       "  0.29239573383027506,\n",
       "  0.29239573383027467,\n",
       "  0.29239573383027445,\n",
       "  0.29239573383027395,\n",
       "  0.29239573383027412,\n",
       "  0.29239573383027412,\n",
       "  0.2923957338302744,\n",
       "  0.29239573383027445,\n",
       "  0.29239573383027445,\n",
       "  0.29239573383027423,\n",
       "  0.29239573383027451,\n",
       "  0.29239573383027467,\n",
       "  0.2923957338302749,\n",
       "  0.29239573383027467,\n",
       "  0.29239573383027423,\n",
       "  0.29239573383027423,\n",
       "  0.29239573383027478,\n",
       "  0.29239573383027473,\n",
       "  0.29239573383027467,\n",
       "  0.29239573383027484,\n",
       "  0.29239573383027406,\n",
       "  0.29239573383027467,\n",
       "  0.29239573383027406,\n",
       "  0.29239573383027412,\n",
       "  0.29239573383027412,\n",
       "  0.29239573383027445,\n",
       "  0.29239573383027428,\n",
       "  0.29239573383027456,\n",
       "  0.29239573383027467,\n",
       "  0.29239573383027423,\n",
       "  0.29239573383027434,\n",
       "  0.29239573383027456,\n",
       "  0.29239573383027467,\n",
       "  0.2923957338302749,\n",
       "  0.29239573383027473,\n",
       "  0.29239573383027473,\n",
       "  0.29239573383027412,\n",
       "  0.2923957338302739,\n",
       "  0.29239573383027406,\n",
       "  0.29239573383027401,\n",
       "  0.29239488528194812,\n",
       "  0.29239488509911982,\n",
       "  0.29239488491638282,\n",
       "  0.29239488473372383,\n",
       "  0.29239488455113205,\n",
       "  0.29239488436859873,\n",
       "  0.29239488418611514,\n",
       "  0.29239488400367425,\n",
       "  0.29239488382127082,\n",
       "  0.29239488363890026,\n",
       "  0.29239403308163497,\n",
       "  0.29239403235160794,\n",
       "  0.29239403162179656,\n",
       "  0.29239403089217458,\n",
       "  0.29239403016271881,\n",
       "  0.29239402943340975,\n",
       "  0.29239402870422965,\n",
       "  0.29239317504488449,\n",
       "  0.29239231992586107,\n",
       "  0.2923923170076223,\n",
       "  0.2923923140902866,\n",
       "  0.29239059983840876,\n",
       "  0.2923905932738699,\n",
       "  0.29238972885448794,\n",
       "  0.29238971992358059,\n",
       "  0.29238971099591443,\n",
       "  0.29238970207123643,\n",
       "  0.29238796722448956,\n",
       "  0.29238535866610055,\n",
       "  0.29238533244209647,\n",
       "  0.29238356827870327,\n",
       "  0.29238266108423233,\n",
       "  0.29237999738383252,\n",
       "  0.29237730586018601,\n",
       "  0.29237370027213733,\n",
       "  0.29237181489927883,\n",
       "  0.2923663376701322,\n",
       "  0.29236255281416673,\n",
       "  0.29235597992563772,\n",
       "  0.29235107164529262,\n",
       "  0.29234051294846086,\n",
       "  0.29233334343176404,\n",
       "  0.29232404294239539,\n",
       "  0.29231821657665363,\n",
       "  0.2923112063779208,\n",
       "  0.29230293642541777,\n",
       "  0.29229131185944851,\n",
       "  0.29227812790946572,\n",
       "  0.29226533645304625,\n",
       "  0.29224977081756315,\n",
       "  0.29223336378198755,\n",
       "  0.29220934662265385,\n",
       "  0.29218495606915429,\n",
       "  0.29216252147710198,\n",
       "  0.29213141590948771,\n",
       "  0.29209313086397581,\n",
       "  0.29203904062665603,\n",
       "  0.29199643624718807,\n",
       "  0.29189636167341032,\n",
       "  0.29182505604311282,\n",
       "  0.29172152345141938,\n",
       "  0.29161045552415049,\n",
       "  0.2914629390582581,\n",
       "  0.29126359842878158,\n",
       "  0.29100440964339408,\n",
       "  0.29071272722081198,\n",
       "  0.29030501004592157,\n",
       "  0.28987122441864033,\n",
       "  0.28930290049387108,\n",
       "  0.2885789221395702,\n",
       "  0.28774831159350039,\n",
       "  0.28662372748716269,\n",
       "  0.28525182374232111,\n",
       "  0.28368404706169281,\n",
       "  0.28182871004930887,\n",
       "  0.27954658327669557,\n",
       "  0.27691972152504218,\n",
       "  0.27420027416630877,\n",
       "  0.27100062119272439,\n",
       "  0.26764535045328858,\n",
       "  0.26438873101175286,\n",
       "  0.26067782638268316,\n",
       "  0.25682506716553444,\n",
       "  0.25235173940923522,\n",
       "  0.24855799627311595,\n",
       "  0.24466247878936298,\n",
       "  0.24090698119099418,\n",
       "  0.23736257494814256,\n",
       "  0.23312769837983185,\n",
       "  0.22945683506862402,\n",
       "  0.22531768235764582,\n",
       "  0.22210036519889914,\n",
       "  0.21757770094851683,\n",
       "  0.21557803357372632,\n",
       "  0.20978649477197289,\n",
       "  0.2132620427974081,\n",
       "  0.2048720433895313,\n",
       "  0.21830761102498286,\n",
       "  0.20551502064673688,\n",
       "  0.2264604924941864,\n",
       "  0.20119993667403233,\n",
       "  0.21499481957696714,\n",
       "  0.19500747448388461,\n",
       "  0.20377610834274695,\n",
       "  0.1890429992459598,\n",
       "  0.19885950820000842,\n",
       "  0.18543883492942817,\n",
       "  0.19490331269853725,\n",
       "  0.18219903785230557,\n",
       "  0.19085888508082097,\n",
       "  0.17817035979041801,\n",
       "  0.18715944598732076,\n",
       "  0.17381666070727214,\n",
       "  0.18233388425123812,\n",
       "  0.17020322313683534,\n",
       "  0.17881610188291702,\n",
       "  0.16731388677742445,\n",
       "  0.17399876980019074,\n",
       "  0.16267931416865947,\n",
       "  0.17051969994623845,\n",
       "  0.16031134488791898,\n",
       "  0.16852908078964002,\n",
       "  0.15805749061599858,\n",
       "  0.16633328352821908,\n",
       "  0.15554031385211389,\n",
       "  0.16465459220664905,\n",
       "  0.15328192865974907,\n",
       "  0.15932304227050004,\n",
       "  0.14765020220772518,\n",
       "  0.14976059220649307,\n",
       "  0.1394084400375229,\n",
       "  0.13951451822536123,\n",
       "  0.13233843244224247,\n",
       "  0.13125057178348418,\n",
       "  0.12719442666221759,\n",
       "  0.12685175658898143,\n",
       "  0.1233700492917448,\n",
       "  0.12295464037575883,\n",
       "  0.11967426649528567,\n",
       "  0.12006163762791512,\n",
       "  0.1183531945493773,\n",
       "  0.1198329382830329,\n",
       "  0.11755700757039581,\n",
       "  0.12440922623430777,\n",
       "  0.12718136158026097,\n",
       "  0.14848925219371747,\n",
       "  0.17892811228247393,\n",
       "  0.20003419257443691,\n",
       "  0.22760284855367013,\n",
       "  0.20311073917934475,\n",
       "  0.16497311992027275,\n",
       "  0.13253805311770273,\n",
       "  0.11283102897784851,\n",
       "  0.10704133340106413,\n",
       "  0.1022538885802565,\n",
       "  0.1022457006954164,\n",
       "  0.09901798502595216,\n",
       "  0.099092058900514229,\n",
       "  0.096941638051429149,\n",
       "  0.096118874319466863,\n",
       "  0.09467545338707975,\n",
       "  0.094115736250648868,\n",
       "  0.092311611689704753,\n",
       "  0.091642872548660076,\n",
       "  0.089751392313412434,\n",
       "  0.088926601957210777,\n",
       "  0.087790165571052028,\n",
       "  0.087038255888396587,\n",
       "  0.085273950856700198,\n",
       "  0.08517234742753485,\n",
       "  0.083318332173584017,\n",
       "  0.083257861399573238,\n",
       "  0.081257662241520534,\n",
       "  0.081617974401657897,\n",
       "  0.079082128098790616,\n",
       "  0.080965833540085352,\n",
       "  0.07855506300322887,\n",
       "  0.079543450201623275,\n",
       "  0.078282500075888786,\n",
       "  0.081162536526446658,\n",
       "  0.082203733340078203,\n",
       "  0.087033107929514311,\n",
       "  0.095756225207295542,\n",
       "  0.10097614921361317,\n",
       "  0.13217224686089923,\n",
       "  0.15195490645914883,\n",
       "  0.25392154588338156,\n",
       "  0.21075641313004906,\n",
       "  0.24116814507758888,\n",
       "  0.14876296733611266,\n",
       "  0.11650306739394309,\n",
       "  0.085407973474158072,\n",
       "  0.076108460438770592,\n",
       "  0.074465848881527616,\n",
       "  0.073848158909661038,\n",
       "  0.071917655056385352,\n",
       "  0.070777866491695499,\n",
       "  0.07011648325869807,\n",
       "  0.069549536143893725,\n",
       "  0.068832093008118475,\n",
       "  0.067919322621040112,\n",
       "  0.066588790715344362,\n",
       "  0.066358883193185517,\n",
       "  0.064891878046214116,\n",
       "  0.064176519196196741,\n",
       "  0.064120205855612344,\n",
       "  0.062500390091643862,\n",
       "  0.062445842196823691,\n",
       "  0.061375049376165516,\n",
       "  0.061150272150832664,\n",
       "  0.060359603277979526,\n",
       "  0.061163713070555313,\n",
       "  0.059603020244075856,\n",
       "  0.060022702034414725,\n",
       "  0.058323050307156524,\n",
       "  0.058872689048872696,\n",
       "  0.057065866160732504,\n",
       "  0.058219337799360761,\n",
       "  0.056040408753044703,\n",
       "  0.056392335368459764,\n",
       "  0.055098939264280111,\n",
       "  0.055130656876851113,\n",
       "  0.054244262640967232,\n",
       "  0.053875606865950269,\n",
       "  0.052894117254617901,\n",
       "  0.05267434009227507,\n",
       "  0.05202832005808293,\n",
       "  0.051794048318161715,\n",
       "  0.052202860003884681,\n",
       "  0.051914181724892781,\n",
       "  0.05175289543950512,\n",
       "  0.051956892002963814,\n",
       "  0.052266770801462657,\n",
       "  0.05298154386340341,\n",
       "  0.055339530767998732,\n",
       "  0.063147278408040339,\n",
       "  0.063477724541742275,\n",
       "  0.087015633702777678,\n",
       "  0.088956215667604172,\n",
       "  0.17133173902942442,\n",
       "  0.15759294310395344,\n",
       "  0.30258380725857836,\n",
       "  0.2455976911642743,\n",
       "  0.26600317219070524,\n",
       "  0.14962837977427756,\n",
       "  0.17321271091535087,\n",
       "  0.07374649312814506,\n",
       "  0.057726431421863331,\n",
       "  0.053200992280848521,\n",
       "  0.050074703634529781,\n",
       "  0.048758478800041062,\n",
       "  0.047709284410901866,\n",
       "  0.045409568747750539,\n",
       "  0.044950594622660892,\n",
       "  0.044098798731343709,\n",
       "  0.043309986712197747,\n",
       "  0.042019223149058184,\n",
       "  0.040906283588224721],\n",
       " [0.08666666666666667,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91433333333333333,\n",
       "  0.91616666666666668,\n",
       "  0.97683333333333333,\n",
       "  0.97550000000000003,\n",
       "  0.97666666666666668,\n",
       "  0.97650000000000003,\n",
       "  0.97766666666666668,\n",
       "  0.97783333333333333,\n",
       "  0.97883333333333333,\n",
       "  0.97883333333333333,\n",
       "  0.97966666666666669,\n",
       "  0.97950000000000004,\n",
       "  0.97999999999999998,\n",
       "  0.97966666666666669,\n",
       "  0.98083333333333333,\n",
       "  0.98033333333333328,\n",
       "  0.98116666666666663,\n",
       "  0.98099999999999998,\n",
       "  0.98166666666666669,\n",
       "  0.98150000000000004,\n",
       "  0.98199999999999998,\n",
       "  0.98133333333333328,\n",
       "  0.98166666666666669,\n",
       "  0.98216666666666663,\n",
       "  0.98150000000000004,\n",
       "  0.98083333333333333,\n",
       "  0.97933333333333328,\n",
       "  0.97683333333333333,\n",
       "  0.97316666666666662,\n",
       "  0.96883333333333332,\n",
       "  0.96033333333333337,\n",
       "  0.9375,\n",
       "  0.93633333333333335,\n",
       "  0.92116666666666669,\n",
       "  0.93633333333333335,\n",
       "  0.95516666666666672,\n",
       "  0.96316666666666662,\n",
       "  0.98199999999999998,\n",
       "  0.98150000000000004,\n",
       "  0.98283333333333334,\n",
       "  0.98216666666666663,\n",
       "  0.98283333333333334,\n",
       "  0.98316666666666663,\n",
       "  0.98333333333333328,\n",
       "  0.98316666666666663,\n",
       "  0.98333333333333328,\n",
       "  0.98366666666666669,\n",
       "  0.98416666666666663,\n",
       "  0.98433333333333328,\n",
       "  0.98483333333333334,\n",
       "  0.98516666666666663,\n",
       "  0.98483333333333334,\n",
       "  0.98550000000000004,\n",
       "  0.98499999999999999,\n",
       "  0.98550000000000004,\n",
       "  0.98533333333333328,\n",
       "  0.98616666666666664,\n",
       "  0.98466666666666669,\n",
       "  0.98650000000000004,\n",
       "  0.98466666666666669,\n",
       "  0.98666666666666669,\n",
       "  0.98499999999999999,\n",
       "  0.98666666666666669,\n",
       "  0.98483333333333334,\n",
       "  0.98666666666666669,\n",
       "  0.98533333333333328,\n",
       "  0.98683333333333334,\n",
       "  0.98566666666666669,\n",
       "  0.98699999999999999,\n",
       "  0.98583333333333334,\n",
       "  0.98699999999999999,\n",
       "  0.98566666666666669,\n",
       "  0.98699999999999999,\n",
       "  0.98566666666666669,\n",
       "  0.98616666666666664,\n",
       "  0.98633333333333328,\n",
       "  0.98633333333333328,\n",
       "  0.98599999999999999,\n",
       "  0.98533333333333328,\n",
       "  0.98450000000000004,\n",
       "  0.98416666666666663,\n",
       "  0.98150000000000004,\n",
       "  0.97950000000000004,\n",
       "  0.97333333333333338,\n",
       "  0.96833333333333338,\n",
       "  0.95183333333333331,\n",
       "  0.94666666666666666,\n",
       "  0.92983333333333329,\n",
       "  0.93466666666666665,\n",
       "  0.9385,\n",
       "  0.95499999999999996,\n",
       "  0.94883333333333331,\n",
       "  0.98299999999999998,\n",
       "  0.98150000000000004,\n",
       "  0.98533333333333328,\n",
       "  0.98666666666666669,\n",
       "  0.98616666666666664,\n",
       "  0.98616666666666664,\n",
       "  0.98733333333333329,\n",
       "  0.98733333333333329,\n",
       "  0.98766666666666669,\n",
       "  0.98799999999999999,\n",
       "  0.98850000000000005,\n",
       "  0.98916666666666664],\n",
       " [0.089200000000000002,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91080000000000005,\n",
       "  0.91359999999999997,\n",
       "  0.96789999999999998,\n",
       "  0.96640000000000004,\n",
       "  0.96809999999999996,\n",
       "  0.9677,\n",
       "  0.96879999999999999,\n",
       "  0.96930000000000005,\n",
       "  0.97040000000000004,\n",
       "  0.97140000000000004,\n",
       "  0.97030000000000005,\n",
       "  0.97209999999999996,\n",
       "  0.97119999999999995,\n",
       "  0.97260000000000002,\n",
       "  0.97189999999999999,\n",
       "  0.97360000000000002,\n",
       "  0.97230000000000005,\n",
       "  0.97399999999999998,\n",
       "  0.97250000000000003,\n",
       "  0.97399999999999998,\n",
       "  0.97250000000000003,\n",
       "  0.97450000000000003,\n",
       "  0.97189999999999999,\n",
       "  0.97440000000000004,\n",
       "  0.97109999999999996,\n",
       "  0.97409999999999997,\n",
       "  0.96799999999999997,\n",
       "  0.97350000000000003,\n",
       "  0.96130000000000004,\n",
       "  0.96870000000000001,\n",
       "  0.94720000000000004,\n",
       "  0.92669999999999997,\n",
       "  0.92330000000000001,\n",
       "  0.91439999999999999,\n",
       "  0.92420000000000002,\n",
       "  0.95820000000000005,\n",
       "  0.95250000000000001,\n",
       "  0.97419999999999995,\n",
       "  0.97299999999999998,\n",
       "  0.97540000000000004,\n",
       "  0.9758,\n",
       "  0.97529999999999994,\n",
       "  0.97550000000000003,\n",
       "  0.97570000000000001,\n",
       "  0.9758,\n",
       "  0.9758,\n",
       "  0.97609999999999997,\n",
       "  0.97609999999999997,\n",
       "  0.97599999999999998,\n",
       "  0.97629999999999995,\n",
       "  0.97629999999999995,\n",
       "  0.97660000000000002,\n",
       "  0.9768,\n",
       "  0.97670000000000001,\n",
       "  0.97719999999999996,\n",
       "  0.97660000000000002,\n",
       "  0.97760000000000002,\n",
       "  0.97660000000000002,\n",
       "  0.97719999999999996,\n",
       "  0.97629999999999995,\n",
       "  0.97729999999999995,\n",
       "  0.97650000000000003,\n",
       "  0.97740000000000005,\n",
       "  0.97660000000000002,\n",
       "  0.97740000000000005,\n",
       "  0.97670000000000001,\n",
       "  0.97760000000000002,\n",
       "  0.9768,\n",
       "  0.97789999999999999,\n",
       "  0.97699999999999998,\n",
       "  0.97789999999999999,\n",
       "  0.97719999999999996,\n",
       "  0.97789999999999999,\n",
       "  0.97719999999999996,\n",
       "  0.97919999999999996,\n",
       "  0.97660000000000002,\n",
       "  0.97840000000000005,\n",
       "  0.97599999999999998,\n",
       "  0.9778,\n",
       "  0.9748,\n",
       "  0.97670000000000001,\n",
       "  0.97250000000000003,\n",
       "  0.97409999999999997,\n",
       "  0.96409999999999996,\n",
       "  0.96419999999999995,\n",
       "  0.94420000000000004,\n",
       "  0.9375,\n",
       "  0.9234,\n",
       "  0.95720000000000005,\n",
       "  0.93130000000000002,\n",
       "  0.9405,\n",
       "  0.9405,\n",
       "  0.97450000000000003,\n",
       "  0.97370000000000001,\n",
       "  0.9778,\n",
       "  0.97789999999999999,\n",
       "  0.97850000000000004,\n",
       "  0.97870000000000001,\n",
       "  0.97899999999999998,\n",
       "  0.97909999999999997,\n",
       "  0.97909999999999997,\n",
       "  0.9798,\n",
       "  0.97960000000000003,\n",
       "  0.9798]]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " net.train(images, labels, iters, alpha, silent=False, verbose=True,\n",
    "        errors=True, tX=tImages, tY=tLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nimages, nlabels = mndata.load_testing()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9798"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.test(tImages,tLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "number = 8\n",
    "plt.clf()\n",
    "plt.imshow(np.reshape(nimages[number], (28,28)), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(net.predict(tImages[:,[number]]))\n",
    "print(\"Is not a 5\") if toBinary(net.predict(tImages[:,[number]]))[0][0] == 0 else print(\"Is a 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAP8AAAD8CAYAAAC4nHJkAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f2fee4dd710>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.59568207]]\n",
      "Is a 5\n"
     ]
    }
   ],
   "source": [
    "number = -2\n",
    "plt.clf()\n",
    "plt.imshow(np.reshape(nimages[number], (28,28)), cmap=\"gray\")\n",
    "plt.show()\n",
    "print(net.predict(tImages[:,[number]]))\n",
    "print(\"Is not a 5\") if toBinary(net.predict(tImages[:,[number]]))[0][0] == 0 else print(\"Is a 5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "signet = NeuralNet.buil23d([784,5,1], \"tanh\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0 of 500\n",
      "Cost is 0.734214257429\n",
      "Iteration 1 of 500\n",
      "Cost is 0.471130610482\n",
      "Iteration 2 of 500\n",
      "Cost is 0.326811943916\n",
      "Iteration 3 of 500\n",
      "Cost is 0.291668047146\n",
      "Iteration 4 of 500\n",
      "Cost is 0.281740342759\n",
      "Iteration 5 of 500\n",
      "Cost is 0.275637328796\n",
      "Iteration 6 of 500\n",
      "Cost is 0.269011155145\n",
      "Iteration 7 of 500\n",
      "Cost is 0.260693263481\n",
      "Iteration 8 of 500\n",
      "Cost is 0.250954983817\n",
      "Iteration 9 of 500\n",
      "Cost is 0.240723548524\n",
      "Iteration 10 of 500\n",
      "Cost is 0.230505696466\n",
      "Iteration 11 of 500\n",
      "Cost is 0.220567110578\n",
      "Iteration 12 of 500\n",
      "Cost is 0.211051380848\n",
      "Iteration 13 of 500\n",
      "Cost is 0.202023239831\n",
      "Iteration 14 of 500\n",
      "Cost is 0.193514199804\n",
      "Iteration 15 of 500\n",
      "Cost is 0.185534798548\n",
      "Iteration 16 of 500\n",
      "Cost is 0.178082330351\n",
      "Iteration 17 of 500\n",
      "Cost is 0.171146054477\n",
      "Iteration 18 of 500\n",
      "Cost is 0.164710032871\n",
      "Iteration 19 of 500\n",
      "Cost is 0.158753486753\n",
      "Iteration 20 of 500\n",
      "Cost is 0.153251554432\n",
      "Iteration 21 of 500\n",
      "Cost is 0.148175628027\n",
      "Iteration 22 of 500\n",
      "Cost is 0.143495116484\n",
      "Iteration 23 of 500\n",
      "Cost is 0.139178051945\n",
      "Iteration 24 of 500\n",
      "Cost is 0.135193365677\n",
      "Iteration 25 of 500\n",
      "Cost is 0.131510551022\n",
      "Iteration 26 of 500\n",
      "Cost is 0.12810290633\n",
      "Iteration 27 of 500\n",
      "Cost is 0.124946748359\n",
      "Iteration 28 of 500\n",
      "Cost is 0.122032704845\n",
      "Iteration 29 of 500\n",
      "Cost is 0.119384854433\n",
      "Iteration 30 of 500\n",
      "Cost is 0.117148400487\n",
      "Iteration 31 of 500\n",
      "Cost is 0.115982009276\n",
      "Iteration 32 of 500\n",
      "Cost is 0.117720740941\n",
      "Iteration 33 of 500\n",
      "Cost is 0.132956327719\n",
      "Iteration 34 of 500\n",
      "Cost is 0.161907224445\n",
      "Iteration 35 of 500\n",
      "Cost is 0.293070293721\n",
      "Iteration 36 of 500\n",
      "Cost is 0.117946718961\n",
      "Iteration 37 of 500\n",
      "Cost is 0.109834504465\n",
      "Iteration 38 of 500\n",
      "Cost is 0.107505529481\n",
      "Iteration 39 of 500\n",
      "Cost is 0.107422321532\n",
      "Iteration 40 of 500\n",
      "Cost is 0.113262422227\n",
      "Iteration 41 of 500\n",
      "Cost is 0.116517069204\n",
      "Iteration 42 of 500\n",
      "Cost is 0.143547443161\n",
      "Iteration 43 of 500\n",
      "Cost is 0.128816214992\n",
      "Iteration 44 of 500\n",
      "Cost is 0.169630964867\n",
      "Iteration 45 of 500\n",
      "Cost is 0.114244657197\n",
      "Iteration 46 of 500\n",
      "Cost is 0.122637619784\n",
      "Iteration 47 of 500\n",
      "Cost is 0.106443988068\n",
      "Iteration 48 of 500\n",
      "Cost is 0.111997357939\n",
      "Iteration 49 of 500\n",
      "Cost is 0.100968095479\n",
      "Iteration 50 of 500\n",
      "Cost is 0.104055253349\n",
      "Iteration 51 of 500\n",
      "Cost is 0.0961853189596\n",
      "Iteration 52 of 500\n",
      "Cost is 0.0977579044468\n",
      "Iteration 53 of 500\n",
      "Cost is 0.0920561937098\n",
      "Iteration 54 of 500\n",
      "Cost is 0.0927253890368\n",
      "Iteration 55 of 500\n",
      "Cost is 0.0885120214111\n",
      "Iteration 56 of 500\n",
      "Cost is 0.0886572566547\n",
      "Iteration 57 of 500\n",
      "Cost is 0.0854557439465\n",
      "Iteration 58 of 500\n",
      "Cost is 0.0853004653203\n",
      "Iteration 59 of 500\n",
      "Cost is 0.0827895656706\n",
      "Iteration 60 of 500\n",
      "Cost is 0.0824619264855\n",
      "Iteration 61 of 500\n",
      "Cost is 0.0804302730026\n",
      "Iteration 62 of 500\n",
      "Cost is 0.0800032100271\n",
      "Iteration 63 of 500\n",
      "Cost is 0.0783122380065\n",
      "Iteration 64 of 500\n",
      "Cost is 0.0778274190188\n",
      "Iteration 65 of 500\n",
      "Cost is 0.0763853722422\n",
      "Iteration 66 of 500\n",
      "Cost is 0.0758669464617\n",
      "Iteration 67 of 500\n",
      "Cost is 0.0746120105619\n",
      "Iteration 68 of 500\n",
      "Cost is 0.0740743887942\n",
      "Iteration 69 of 500\n",
      "Cost is 0.0729640446551\n",
      "Iteration 70 of 500\n",
      "Cost is 0.0724162594552\n",
      "Iteration 71 of 500\n",
      "Cost is 0.0714205686365\n",
      "Iteration 72 of 500\n",
      "Cost is 0.0708686836464\n",
      "Iteration 73 of 500\n",
      "Cost is 0.0699660129594\n",
      "Iteration 74 of 500\n",
      "Cost is 0.0694144214289\n",
      "Iteration 75 of 500\n",
      "Cost is 0.0685886933384\n",
      "Iteration 76 of 500\n",
      "Cost is 0.0680407881663\n",
      "Iteration 77 of 500\n",
      "Cost is 0.0672797025552\n",
      "Iteration 78 of 500\n",
      "Cost is 0.0667381987411\n",
      "Iteration 79 of 500\n",
      "Cost is 0.0660320824259\n",
      "Iteration 80 of 500\n",
      "Cost is 0.065499158875\n",
      "Iteration 81 of 500\n",
      "Cost is 0.0648402229042\n",
      "Iteration 82 of 500\n",
      "Cost is 0.0643175833208\n",
      "Iteration 83 of 500\n",
      "Cost is 0.0636994428326\n",
      "Iteration 84 of 500\n",
      "Cost is 0.0631883525458\n",
      "Iteration 85 of 500\n",
      "Cost is 0.0626057117753\n",
      "Iteration 86 of 500\n",
      "Cost is 0.0621070382164\n",
      "Iteration 87 of 500\n",
      "Cost is 0.0615554759092\n",
      "Iteration 88 of 500\n",
      "Cost is 0.0610697410687\n",
      "Iteration 89 of 500\n",
      "Cost is 0.0605455548973\n",
      "Iteration 90 of 500\n",
      "Cost is 0.060072996848\n",
      "Iteration 91 of 500\n",
      "Cost is 0.0595730821137\n",
      "Iteration 92 of 500\n",
      "Cost is 0.0591137181708\n",
      "Iteration 93 of 500\n",
      "Cost is 0.0586354674632\n",
      "Iteration 94 of 500\n",
      "Cost is 0.0581891517745\n",
      "Iteration 95 of 500\n",
      "Cost is 0.0577303693657\n",
      "Iteration 96 of 500\n",
      "Cost is 0.0572968404401\n",
      "Iteration 97 of 500\n",
      "Cost is 0.0568556690347\n",
      "Iteration 98 of 500\n",
      "Cost is 0.0564345859684\n",
      "Iteration 99 of 500\n",
      "Cost is 0.0560094450173\n",
      "Iteration 100 of 500\n",
      "Cost is 0.0556004136994\n",
      "Iteration 101 of 500\n",
      "Cost is 0.0551899487549\n",
      "Iteration 102 of 500\n",
      "Cost is 0.0547925406111\n",
      "Iteration 103 of 500\n",
      "Cost is 0.0543955828537\n",
      "Iteration 104 of 500\n",
      "Cost is 0.0540093488093\n",
      "Iteration 105 of 500\n",
      "Cost is 0.0536248834229\n",
      "Iteration 106 of 500\n",
      "Cost is 0.05324936514\n",
      "Iteration 107 of 500\n",
      "Cost is 0.0528765068959\n",
      "Iteration 108 of 500\n",
      "Cost is 0.0525112464532\n",
      "Iteration 109 of 500\n",
      "Cost is 0.0521492207954\n",
      "Iteration 110 of 500\n",
      "Cost is 0.0517937691902\n",
      "Iteration 111 of 500\n",
      "Cost is 0.0514418972612\n",
      "Iteration 112 of 500\n",
      "Cost is 0.0510958216219\n",
      "Iteration 113 of 500\n",
      "Cost is 0.0507535079527\n",
      "Iteration 114 of 500\n",
      "Cost is 0.050416397173\n",
      "Iteration 115 of 500\n",
      "Cost is 0.0500831191041\n",
      "Iteration 116 of 500\n",
      "Cost is 0.0497545876668\n",
      "Iteration 117 of 500\n",
      "Cost is 0.0494298858901\n",
      "Iteration 118 of 500\n",
      "Cost is 0.0491095758264\n",
      "Iteration 119 of 500\n",
      "Cost is 0.0487930457081\n",
      "Iteration 120 of 500\n",
      "Cost is 0.0484806268375\n",
      "Iteration 121 of 500\n",
      "Cost is 0.048171910378\n",
      "Iteration 122 of 500\n",
      "Cost is 0.0478670791307\n",
      "Iteration 123 of 500\n",
      "Cost is 0.0475658575466\n",
      "Iteration 124 of 500\n",
      "Cost is 0.0472683347571\n",
      "Iteration 125 of 500\n",
      "Cost is 0.0469743217334\n",
      "Iteration 126 of 500\n",
      "Cost is 0.046683849814\n",
      "Iteration 127 of 500\n",
      "Cost is 0.0463967854987\n",
      "Iteration 128 of 500\n",
      "Cost is 0.0461131253629\n",
      "Iteration 129 of 500\n",
      "Cost is 0.0458327711599\n",
      "Iteration 130 of 500\n",
      "Cost is 0.0455556991962\n",
      "Iteration 131 of 500\n",
      "Cost is 0.0452818333825\n",
      "Iteration 132 of 500\n",
      "Cost is 0.0450111386929\n",
      "Iteration 133 of 500\n",
      "Cost is 0.0447435528454\n",
      "Iteration 134 of 500\n",
      "Cost is 0.0444790348731\n",
      "Iteration 135 of 500\n",
      "Cost is 0.044217531052\n",
      "Iteration 136 of 500\n",
      "Cost is 0.0439589976522\n",
      "Iteration 137 of 500\n",
      "Cost is 0.0437033862597\n",
      "Iteration 138 of 500\n",
      "Cost is 0.0434506522075\n",
      "Iteration 139 of 500\n",
      "Cost is 0.0432007504202\n",
      "Iteration 140 of 500\n",
      "Cost is 0.0429536363126\n",
      "Iteration 141 of 500\n",
      "Cost is 0.0427092669807\n",
      "Iteration 142 of 500\n",
      "Cost is 0.0424675984709\n",
      "Iteration 143 of 500\n",
      "Cost is 0.0422285893815\n",
      "Iteration 144 of 500\n",
      "Cost is 0.0419921966747\n",
      "Iteration 145 of 500\n",
      "Cost is 0.0417583800857\n",
      "Iteration 146 of 500\n",
      "Cost is 0.0415270976308\n",
      "Iteration 147 of 500\n",
      "Cost is 0.0412983099981\n",
      "Iteration 148 of 500\n",
      "Cost is 0.0410719763183\n",
      "Iteration 149 of 500\n",
      "Cost is 0.0408480581528\n",
      "Iteration 150 of 500\n",
      "Cost is 0.040626515768\n",
      "Iteration 151 of 500\n",
      "Cost is 0.0404073115765\n",
      "Iteration 152 of 500\n",
      "Cost is 0.0401904069823\n",
      "Iteration 153 of 500\n",
      "Cost is 0.039975765257\n",
      "Iteration 154 of 500\n",
      "Cost is 0.039763348935\n",
      "Iteration 155 of 500\n",
      "Cost is 0.0395531221679\n",
      "Iteration 156 of 500\n",
      "Cost is 0.0393450486105\n",
      "Iteration 157 of 500\n",
      "Cost is 0.0391390933188\n",
      "Iteration 158 of 500\n",
      "Cost is 0.0389352210565\n",
      "Iteration 159 of 500\n",
      "Cost is 0.0387333978081\n",
      "Iteration 160 of 500\n",
      "Cost is 0.0385335894345\n",
      "Iteration 161 of 500\n",
      "Cost is 0.0383357628696\n",
      "Iteration 162 of 500\n",
      "Cost is 0.0381398850606\n",
      "Iteration 163 of 500\n",
      "Cost is 0.0379459239075\n",
      "Iteration 164 of 500\n",
      "Cost is 0.0377538474334\n",
      "Iteration 165 of 500\n",
      "Cost is 0.0375636245177\n",
      "Iteration 166 of 500\n",
      "Cost is 0.0373752242496\n",
      "Iteration 167 of 500\n",
      "Cost is 0.0371886164978\n",
      "Iteration 168 of 500\n",
      "Cost is 0.0370037714091\n",
      "Iteration 169 of 500\n",
      "Cost is 0.0368206598486\n",
      "Iteration 170 of 500\n",
      "Cost is 0.0366392530127\n",
      "Iteration 171 of 500\n",
      "Cost is 0.0364595227684\n",
      "Iteration 172 of 500\n",
      "Cost is 0.036281441355\n",
      "Iteration 173 of 500\n",
      "Cost is 0.0361049816452\n",
      "Iteration 174 of 500\n",
      "Cost is 0.0359301169158\n",
      "Iteration 175 of 500\n",
      "Cost is 0.0357568210475\n",
      "Iteration 176 of 500\n",
      "Cost is 0.0355850683503\n",
      "Iteration 177 of 500\n",
      "Cost is 0.0354148337153\n",
      "Iteration 178 of 500\n",
      "Cost is 0.0352460924819\n",
      "Iteration 179 of 500\n",
      "Cost is 0.0350788205531\n",
      "Iteration 180 of 500\n",
      "Cost is 0.0349129942947\n",
      "Iteration 181 of 500\n",
      "Cost is 0.0347485906227\n",
      "Iteration 182 of 500\n",
      "Cost is 0.0345855869266\n",
      "Iteration 183 of 500\n",
      "Cost is 0.0344239611354\n",
      "Iteration 184 of 500\n",
      "Cost is 0.0342636916593\n",
      "Iteration 185 of 500\n",
      "Cost is 0.0341047574395\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 186 of 500\n",
      "Cost is 0.0339471379031\n",
      "Iteration 187 of 500\n",
      "Cost is 0.0337908130004\n",
      "Iteration 188 of 500\n",
      "Cost is 0.0336357631702\n",
      "Iteration 189 of 500\n",
      "Cost is 0.0334819693667\n",
      "Iteration 190 of 500\n",
      "Cost is 0.0333294130325\n",
      "Iteration 191 of 500\n",
      "Cost is 0.0331780761176\n",
      "Iteration 192 of 500\n",
      "Cost is 0.0330279410572\n",
      "Iteration 193 of 500\n",
      "Cost is 0.0328789907846\n",
      "Iteration 194 of 500\n",
      "Cost is 0.0327312087131\n",
      "Iteration 195 of 500\n",
      "Cost is 0.0325845787423\n",
      "Iteration 196 of 500\n",
      "Cost is 0.0324390852432\n",
      "Iteration 197 of 500\n",
      "Cost is 0.0322947130599\n",
      "Iteration 198 of 500\n",
      "Cost is 0.0321514474955\n",
      "Iteration 199 of 500\n",
      "Cost is 0.0320092743108\n",
      "Iteration 200 of 500\n",
      "Cost is 0.0318681797101\n",
      "Iteration 201 of 500\n",
      "Cost is 0.0317281503364\n",
      "Iteration 202 of 500\n",
      "Cost is 0.0315891732578\n",
      "Iteration 203 of 500\n",
      "Cost is 0.0314512359599\n",
      "Iteration 204 of 500\n",
      "Cost is 0.0313143263314\n",
      "Iteration 205 of 500\n",
      "Cost is 0.0311784326546\n",
      "Iteration 206 of 500\n",
      "Cost is 0.0310435435903\n",
      "Iteration 207 of 500\n",
      "Cost is 0.0309096481665\n",
      "Iteration 208 of 500\n",
      "Cost is 0.0307767357631\n",
      "Iteration 209 of 500\n",
      "Cost is 0.0306447960989\n",
      "Iteration 210 of 500\n",
      "Cost is 0.0305138192158\n",
      "Iteration 211 of 500\n",
      "Cost is 0.0303837954654\n",
      "Iteration 212 of 500\n",
      "Cost is 0.030254715493\n",
      "Iteration 213 of 500\n",
      "Cost is 0.0301265702232\n",
      "Iteration 214 of 500\n",
      "Cost is 0.0299993508444\n",
      "Iteration 215 of 500\n",
      "Cost is 0.0298730487947\n",
      "Iteration 216 of 500\n",
      "Cost is 0.0297476557466\n",
      "Iteration 217 of 500\n",
      "Cost is 0.029623163593\n",
      "Iteration 218 of 500\n",
      "Cost is 0.0294995644329\n",
      "Iteration 219 of 500\n",
      "Cost is 0.0293768505586\n",
      "Iteration 220 of 500\n",
      "Cost is 0.0292550144418\n",
      "Iteration 221 of 500\n",
      "Cost is 0.0291340487218\n",
      "Iteration 222 of 500\n",
      "Cost is 0.0290139461934\n",
      "Iteration 223 of 500\n",
      "Cost is 0.0288946997963\n",
      "Iteration 224 of 500\n",
      "Cost is 0.0287763026037\n",
      "Iteration 225 of 500\n",
      "Cost is 0.0286587478138\n",
      "Iteration 226 of 500\n",
      "Cost is 0.0285420287399\n",
      "Iteration 227 of 500\n",
      "Cost is 0.0284261388028\n",
      "Iteration 228 of 500\n",
      "Cost is 0.0283110715227\n",
      "Iteration 229 of 500\n",
      "Cost is 0.0281968205133\n",
      "Iteration 230 of 500\n",
      "Cost is 0.0280833794748\n",
      "Iteration 231 of 500\n",
      "Cost is 0.0279707421892\n",
      "Iteration 232 of 500\n",
      "Cost is 0.0278589025154\n",
      "Iteration 233 of 500\n",
      "Cost is 0.0277478543846\n",
      "Iteration 234 of 500\n",
      "Cost is 0.0276375917974\n",
      "Iteration 235 of 500\n",
      "Cost is 0.02752810882\n",
      "Iteration 236 of 500\n",
      "Cost is 0.027419399582\n",
      "Iteration 237 of 500\n",
      "Cost is 0.0273114582741\n",
      "Iteration 238 of 500\n",
      "Cost is 0.0272042791459\n",
      "Iteration 239 of 500\n",
      "Cost is 0.0270978565046\n",
      "Iteration 240 of 500\n",
      "Cost is 0.0269921847134\n",
      "Iteration 241 of 500\n",
      "Cost is 0.0268872581903\n",
      "Iteration 242 of 500\n",
      "Cost is 0.0267830714072\n",
      "Iteration 243 of 500\n",
      "Cost is 0.0266796188889\n",
      "Iteration 244 of 500\n",
      "Cost is 0.026576895212\n",
      "Iteration 245 of 500\n",
      "Cost is 0.0264748950042\n",
      "Iteration 246 of 500\n",
      "Cost is 0.0263736129431\n",
      "Iteration 247 of 500\n",
      "Cost is 0.0262730437555\n",
      "Iteration 248 of 500\n",
      "Cost is 0.0261731822162\n",
      "Iteration 249 of 500\n",
      "Cost is 0.0260740231464\n",
      "Iteration 250 of 500\n",
      "Cost is 0.0259755614129\n",
      "Iteration 251 of 500\n",
      "Cost is 0.0258777919264\n",
      "Iteration 252 of 500\n",
      "Cost is 0.0257807096397\n",
      "Iteration 253 of 500\n",
      "Cost is 0.0256843095462\n",
      "Iteration 254 of 500\n",
      "Cost is 0.0255885866775\n",
      "Iteration 255 of 500\n",
      "Cost is 0.0254935361016\n",
      "Iteration 256 of 500\n",
      "Cost is 0.0253991529207\n",
      "Iteration 257 of 500\n",
      "Cost is 0.0253054322684\n",
      "Iteration 258 of 500\n",
      "Cost is 0.0252123693074\n",
      "Iteration 259 of 500\n",
      "Cost is 0.025119959227\n",
      "Iteration 260 of 500\n",
      "Cost is 0.0250281972399\n",
      "Iteration 261 of 500\n",
      "Cost is 0.0249370785798\n",
      "Iteration 262 of 500\n",
      "Cost is 0.0248465984984\n",
      "Iteration 263 of 500\n",
      "Cost is 0.0247567522625\n",
      "Iteration 264 of 500\n",
      "Cost is 0.0246675351511\n",
      "Iteration 265 of 500\n",
      "Cost is 0.0245789424526\n",
      "Iteration 266 of 500\n",
      "Cost is 0.0244909694619\n",
      "Iteration 267 of 500\n",
      "Cost is 0.0244036114778\n",
      "Iteration 268 of 500\n",
      "Cost is 0.0243168637999\n",
      "Iteration 269 of 500\n",
      "Cost is 0.0242307217269\n",
      "Iteration 270 of 500\n",
      "Cost is 0.0241451805532\n",
      "Iteration 271 of 500\n",
      "Cost is 0.0240602355672\n",
      "Iteration 272 of 500\n",
      "Cost is 0.0239758820492\n",
      "Iteration 273 of 500\n",
      "Cost is 0.0238921152692\n",
      "Iteration 274 of 500\n",
      "Cost is 0.0238089304851\n",
      "Iteration 275 of 500\n",
      "Cost is 0.0237263229415\n",
      "Iteration 276 of 500\n",
      "Cost is 0.0236442878679\n",
      "Iteration 277 of 500\n",
      "Cost is 0.023562820478\n",
      "Iteration 278 of 500\n",
      "Cost is 0.023481915968\n",
      "Iteration 279 of 500\n",
      "Cost is 0.0234015695167\n",
      "Iteration 280 of 500\n",
      "Cost is 0.0233217762845\n",
      "Iteration 281 of 500\n",
      "Cost is 0.0232425314127\n",
      "Iteration 282 of 500\n",
      "Cost is 0.0231638300242\n",
      "Iteration 283 of 500\n",
      "Cost is 0.0230856672228\n",
      "Iteration 284 of 500\n",
      "Cost is 0.0230080380936\n",
      "Iteration 285 of 500\n",
      "Cost is 0.0229309377036\n",
      "Iteration 286 of 500\n",
      "Cost is 0.0228543611021\n",
      "Iteration 287 of 500\n",
      "Cost is 0.0227783033213\n",
      "Iteration 288 of 500\n",
      "Cost is 0.0227027593773\n",
      "Iteration 289 of 500\n",
      "Cost is 0.0226277242712\n",
      "Iteration 290 of 500\n",
      "Cost is 0.02255319299\n",
      "Iteration 291 of 500\n",
      "Cost is 0.0224791605081\n",
      "Iteration 292 of 500\n",
      "Cost is 0.0224056217884\n",
      "Iteration 293 of 500\n",
      "Cost is 0.0223325717839\n",
      "Iteration 294 of 500\n",
      "Cost is 0.0222600054393\n",
      "Iteration 295 of 500\n",
      "Cost is 0.0221879176925\n",
      "Iteration 296 of 500\n",
      "Cost is 0.0221163034765\n",
      "Iteration 297 of 500\n",
      "Cost is 0.0220451577209\n",
      "Iteration 298 of 500\n",
      "Cost is 0.0219744753542\n",
      "Iteration 299 of 500\n",
      "Cost is 0.0219042513051\n",
      "Iteration 300 of 500\n",
      "Cost is 0.0218344805048\n",
      "Iteration 301 of 500\n",
      "Cost is 0.0217651578892\n",
      "Iteration 302 of 500\n",
      "Cost is 0.0216962784004\n",
      "Iteration 303 of 500\n",
      "Cost is 0.0216278369891\n",
      "Iteration 304 of 500\n",
      "Cost is 0.0215598286169\n",
      "Iteration 305 of 500\n",
      "Cost is 0.0214922482579\n",
      "Iteration 306 of 500\n",
      "Cost is 0.0214250909015\n",
      "Iteration 307 of 500\n",
      "Cost is 0.021358351554\n",
      "Iteration 308 of 500\n",
      "Cost is 0.0212920252413\n",
      "Iteration 309 of 500\n",
      "Cost is 0.0212261070109\n",
      "Iteration 310 of 500\n",
      "Cost is 0.0211605919345\n",
      "Iteration 311 of 500\n",
      "Cost is 0.0210954751099\n",
      "Iteration 312 of 500\n",
      "Cost is 0.0210307516637\n",
      "Iteration 313 of 500\n",
      "Cost is 0.0209664167536\n",
      "Iteration 314 of 500\n",
      "Cost is 0.0209024655705\n",
      "Iteration 315 of 500\n",
      "Cost is 0.0208388933415\n",
      "Iteration 316 of 500\n",
      "Cost is 0.020775695332\n",
      "Iteration 317 of 500\n",
      "Cost is 0.0207128668483\n",
      "Iteration 318 of 500\n",
      "Cost is 0.0206504032399\n",
      "Iteration 319 of 500\n",
      "Cost is 0.0205882999025\n",
      "Iteration 320 of 500\n",
      "Cost is 0.02052655228\n",
      "Iteration 321 of 500\n",
      "Cost is 0.0204651558675\n",
      "Iteration 322 of 500\n",
      "Cost is 0.0204041062137\n",
      "Iteration 323 of 500\n",
      "Cost is 0.0203433989235\n",
      "Iteration 324 of 500\n",
      "Cost is 0.0202830296606\n",
      "Iteration 325 of 500\n",
      "Cost is 0.0202229941499\n",
      "Iteration 326 of 500\n",
      "Cost is 0.0201632881804\n",
      "Iteration 327 of 500\n",
      "Cost is 0.0201039076072\n",
      "Iteration 328 of 500\n",
      "Cost is 0.0200448483544\n",
      "Iteration 329 of 500\n",
      "Cost is 0.0199861064174\n",
      "Iteration 330 of 500\n",
      "Cost is 0.019927677865\n",
      "Iteration 331 of 500\n",
      "Cost is 0.0198695588421\n",
      "Iteration 332 of 500\n",
      "Cost is 0.0198117455713\n",
      "Iteration 333 of 500\n",
      "Cost is 0.0197542343553\n",
      "Iteration 334 of 500\n",
      "Cost is 0.0196970215788\n",
      "Iteration 335 of 500\n",
      "Cost is 0.0196401037101\n",
      "Iteration 336 of 500\n",
      "Cost is 0.0195834773028\n",
      "Iteration 337 of 500\n",
      "Cost is 0.019527138997\n",
      "Iteration 338 of 500\n",
      "Cost is 0.0194710855208\n",
      "Iteration 339 of 500\n",
      "Cost is 0.019415313691\n",
      "Iteration 340 of 500\n",
      "Cost is 0.0193598204141\n",
      "Iteration 341 of 500\n",
      "Cost is 0.0193046026867\n",
      "Iteration 342 of 500\n",
      "Cost is 0.0192496575957\n",
      "Iteration 343 of 500\n",
      "Cost is 0.0191949823185\n",
      "Iteration 344 of 500\n",
      "Cost is 0.0191405741226\n",
      "Iteration 345 of 500\n",
      "Cost is 0.0190864303648\n",
      "Iteration 346 of 500\n",
      "Cost is 0.0190325484911\n",
      "Iteration 347 of 500\n",
      "Cost is 0.0189789260348\n",
      "Iteration 348 of 500\n",
      "Cost is 0.0189255606158\n",
      "Iteration 349 of 500\n",
      "Cost is 0.0188724499386\n",
      "Iteration 350 of 500\n",
      "Cost is 0.0188195917904\n",
      "Iteration 351 of 500\n",
      "Cost is 0.0187669840394\n",
      "Iteration 352 of 500\n",
      "Cost is 0.0187146246319\n",
      "Iteration 353 of 500\n",
      "Cost is 0.0186625115903\n",
      "Iteration 354 of 500\n",
      "Cost is 0.0186106430101\n",
      "Iteration 355 of 500\n",
      "Cost is 0.0185590170569\n",
      "Iteration 356 of 500\n",
      "Cost is 0.0185076319638\n",
      "Iteration 357 of 500\n",
      "Cost is 0.0184564860278\n",
      "Iteration 358 of 500\n",
      "Cost is 0.0184055776069\n",
      "Iteration 359 of 500\n",
      "Cost is 0.0183549051167\n",
      "Iteration 360 of 500\n",
      "Cost is 0.0183044670272\n",
      "Iteration 361 of 500\n",
      "Cost is 0.0182542618593\n",
      "Iteration 362 of 500\n",
      "Cost is 0.0182042881818\n",
      "Iteration 363 of 500\n",
      "Cost is 0.0181545446079\n",
      "Iteration 364 of 500\n",
      "Cost is 0.0181050297921\n",
      "Iteration 365 of 500\n",
      "Cost is 0.0180557424272\n",
      "Iteration 366 of 500\n",
      "Cost is 0.0180066812413\n",
      "Iteration 367 of 500\n",
      "Cost is 0.0179578449948\n",
      "Iteration 368 of 500\n",
      "Cost is 0.0179092324778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 369 of 500\n",
      "Cost is 0.0178608425077\n",
      "Iteration 370 of 500\n",
      "Cost is 0.0178126739263\n",
      "Iteration 371 of 500\n",
      "Cost is 0.0177647255979\n",
      "Iteration 372 of 500\n",
      "Cost is 0.0177169964072\n",
      "Iteration 373 of 500\n",
      "Cost is 0.0176694852572\n",
      "Iteration 374 of 500\n",
      "Cost is 0.0176221910675\n",
      "Iteration 375 of 500\n",
      "Cost is 0.0175751127726\n",
      "Iteration 376 of 500\n",
      "Cost is 0.0175282493205\n",
      "Iteration 377 of 500\n",
      "Cost is 0.0174815996713\n",
      "Iteration 378 of 500\n",
      "Cost is 0.0174351627961\n",
      "Iteration 379 of 500\n",
      "Cost is 0.0173889376759\n",
      "Iteration 380 of 500\n",
      "Cost is 0.0173429233003\n",
      "Iteration 381 of 500\n",
      "Cost is 0.0172971186673\n",
      "Iteration 382 of 500\n",
      "Cost is 0.0172515227821\n",
      "Iteration 383 of 500\n",
      "Cost is 0.0172061346562\n",
      "Iteration 384 of 500\n",
      "Cost is 0.0171609533076\n",
      "Iteration 385 of 500\n",
      "Cost is 0.0171159777592\n",
      "Iteration 386 of 500\n",
      "Cost is 0.0170712070394\n",
      "Iteration 387 of 500\n",
      "Cost is 0.0170266401806\n",
      "Iteration 388 of 500\n",
      "Cost is 0.0169822762196\n",
      "Iteration 389 of 500\n",
      "Cost is 0.0169381141968\n",
      "Iteration 390 of 500\n",
      "Cost is 0.0168941531561\n",
      "Iteration 391 of 500\n",
      "Cost is 0.016850392144\n",
      "Iteration 392 of 500\n",
      "Cost is 0.0168068302101\n",
      "Iteration 393 of 500\n",
      "Cost is 0.0167634664058\n",
      "Iteration 394 of 500\n",
      "Cost is 0.0167202997849\n",
      "Iteration 395 of 500\n",
      "Cost is 0.0166773294023\n",
      "Iteration 396 of 500\n",
      "Cost is 0.0166345543145\n",
      "Iteration 397 of 500\n",
      "Cost is 0.0165919735785\n",
      "Iteration 398 of 500\n",
      "Cost is 0.016549586252\n",
      "Iteration 399 of 500\n",
      "Cost is 0.0165073913925\n",
      "Iteration 400 of 500\n",
      "Cost is 0.0164653880574\n",
      "Iteration 401 of 500\n",
      "Cost is 0.0164235753029\n",
      "Iteration 402 of 500\n",
      "Cost is 0.0163819521844\n",
      "Iteration 403 of 500\n",
      "Cost is 0.0163405177552\n",
      "Iteration 404 of 500\n",
      "Cost is 0.0162992710667\n",
      "Iteration 405 of 500\n",
      "Cost is 0.0162582111675\n",
      "Iteration 406 of 500\n",
      "Cost is 0.0162173371031\n",
      "Iteration 407 of 500\n",
      "Cost is 0.0161766479153\n",
      "Iteration 408 of 500\n",
      "Cost is 0.0161361426419\n",
      "Iteration 409 of 500\n",
      "Cost is 0.016095820316\n",
      "Iteration 410 of 500\n",
      "Cost is 0.0160556799654\n",
      "Iteration 411 of 500\n",
      "Cost is 0.0160157206125\n",
      "Iteration 412 of 500\n",
      "Cost is 0.0159759412734\n",
      "Iteration 413 of 500\n",
      "Cost is 0.0159363409574\n",
      "Iteration 414 of 500\n",
      "Cost is 0.0158969186666\n",
      "Iteration 415 of 500\n",
      "Cost is 0.0158576733957\n",
      "Iteration 416 of 500\n",
      "Cost is 0.015818604131\n",
      "Iteration 417 of 500\n",
      "Cost is 0.01577970985\n",
      "Iteration 418 of 500\n",
      "Cost is 0.0157409895213\n",
      "Iteration 419 of 500\n",
      "Cost is 0.0157024421037\n",
      "Iteration 420 of 500\n",
      "Cost is 0.0156640665461\n",
      "Iteration 421 of 500\n",
      "Cost is 0.0156258617867\n",
      "Iteration 422 of 500\n",
      "Cost is 0.015587826753\n",
      "Iteration 423 of 500\n",
      "Cost is 0.015549960361\n",
      "Iteration 424 of 500\n",
      "Cost is 0.0155122615151\n",
      "Iteration 425 of 500\n",
      "Cost is 0.0154747291074\n",
      "Iteration 426 of 500\n",
      "Cost is 0.0154373620179\n",
      "Iteration 427 of 500\n",
      "Cost is 0.0154001591136\n",
      "Iteration 428 of 500\n",
      "Cost is 0.0153631192484\n",
      "Iteration 429 of 500\n",
      "Cost is 0.0153262412631\n",
      "Iteration 430 of 500\n",
      "Cost is 0.0152895239848\n",
      "Iteration 431 of 500\n",
      "Cost is 0.0152529662265\n",
      "Iteration 432 of 500\n",
      "Cost is 0.0152165667874\n",
      "Iteration 433 of 500\n",
      "Cost is 0.0151803244524\n",
      "Iteration 434 of 500\n",
      "Cost is 0.015144237992\n",
      "Iteration 435 of 500\n",
      "Cost is 0.0151083061621\n",
      "Iteration 436 of 500\n",
      "Cost is 0.015072527704\n",
      "Iteration 437 of 500\n",
      "Cost is 0.0150369013443\n",
      "Iteration 438 of 500\n",
      "Cost is 0.0150014257946\n",
      "Iteration 439 of 500\n",
      "Cost is 0.0149660997519\n",
      "Iteration 440 of 500\n",
      "Cost is 0.0149309218983\n",
      "Iteration 441 of 500\n",
      "Cost is 0.0148958909011\n",
      "Iteration 442 of 500\n",
      "Cost is 0.0148610054129\n",
      "Iteration 443 of 500\n",
      "Cost is 0.0148262640716\n",
      "Iteration 444 of 500\n",
      "Cost is 0.0147916655008\n",
      "Iteration 445 of 500\n",
      "Cost is 0.0147572083093\n",
      "Iteration 446 of 500\n",
      "Cost is 0.0147228910921\n",
      "Iteration 447 of 500\n",
      "Cost is 0.0146887124301\n",
      "Iteration 448 of 500\n",
      "Cost is 0.0146546708905\n",
      "Iteration 449 of 500\n",
      "Cost is 0.014620765027\n",
      "Iteration 450 of 500\n",
      "Cost is 0.0145869933803\n",
      "Iteration 451 of 500\n",
      "Cost is 0.0145533544782\n",
      "Iteration 452 of 500\n",
      "Cost is 0.0145198468362\n",
      "Iteration 453 of 500\n",
      "Cost is 0.0144864689581\n",
      "Iteration 454 of 500\n",
      "Cost is 0.0144532193359\n",
      "Iteration 455 of 500\n",
      "Cost is 0.0144200964509\n",
      "Iteration 456 of 500\n",
      "Cost is 0.0143870987741\n",
      "Iteration 457 of 500\n",
      "Cost is 0.0143542247668\n",
      "Iteration 458 of 500\n",
      "Cost is 0.0143214728811\n",
      "Iteration 459 of 500\n",
      "Cost is 0.014288841561\n",
      "Iteration 460 of 500\n",
      "Cost is 0.0142563292428\n",
      "Iteration 461 of 500\n",
      "Cost is 0.0142239343565\n",
      "Iteration 462 of 500\n",
      "Cost is 0.0141916553261\n",
      "Iteration 463 of 500\n",
      "Cost is 0.0141594905709\n",
      "Iteration 464 of 500\n",
      "Cost is 0.0141274385066\n",
      "Iteration 465 of 500\n",
      "Cost is 0.0140954975466\n",
      "Iteration 466 of 500\n",
      "Cost is 0.0140636661029\n",
      "Iteration 467 of 500\n",
      "Cost is 0.0140319425874\n",
      "Iteration 468 of 500\n",
      "Cost is 0.0140003254139\n",
      "Iteration 469 of 500\n",
      "Cost is 0.0139688129992\n",
      "Iteration 470 of 500\n",
      "Cost is 0.0139374037646\n",
      "Iteration 471 of 500\n",
      "Cost is 0.0139060961381\n",
      "Iteration 472 of 500\n",
      "Cost is 0.013874888556\n",
      "Iteration 473 of 500\n",
      "Cost is 0.0138437794652\n",
      "Iteration 474 of 500\n",
      "Cost is 0.0138127673248\n",
      "Iteration 475 of 500\n",
      "Cost is 0.0137818506092\n",
      "Iteration 476 of 500\n",
      "Cost is 0.0137510278098\n",
      "Iteration 477 of 500\n",
      "Cost is 0.0137202974382\n",
      "Iteration 478 of 500\n",
      "Cost is 0.0136896580284\n",
      "Iteration 479 of 500\n",
      "Cost is 0.0136591081403\n",
      "Iteration 480 of 500\n",
      "Cost is 0.0136286463623\n",
      "Iteration 481 of 500\n",
      "Cost is 0.0135982713148\n",
      "Iteration 482 of 500\n",
      "Cost is 0.0135679816536\n",
      "Iteration 483 of 500\n",
      "Cost is 0.0135377760735\n",
      "Iteration 484 of 500\n",
      "Cost is 0.0135076533119\n",
      "Iteration 485 of 500\n",
      "Cost is 0.0134776121527\n",
      "Iteration 486 of 500\n",
      "Cost is 0.0134476514304\n",
      "Iteration 487 of 500\n",
      "Cost is 0.0134177700344\n",
      "Iteration 488 of 500\n",
      "Cost is 0.0133879669129\n",
      "Iteration 489 of 500\n",
      "Cost is 0.0133582410774\n",
      "Iteration 490 of 500\n",
      "Cost is 0.0133285916074\n",
      "Iteration 491 of 500\n",
      "Cost is 0.0132990176543\n",
      "Iteration 492 of 500\n",
      "Cost is 0.0132695184465\n",
      "Iteration 493 of 500\n",
      "Cost is 0.0132400932931\n",
      "Iteration 494 of 500\n",
      "Cost is 0.0132107415891\n",
      "Iteration 495 of 500\n",
      "Cost is 0.0131814628189\n",
      "Iteration 496 of 500\n",
      "Cost is 0.0131522565608\n",
      "Iteration 497 of 500\n",
      "Cost is 0.0131231224909\n",
      "Iteration 498 of 500\n",
      "Cost is 0.0130940603864\n",
      "Iteration 499 of 500\n",
      "Cost is 0.0130650701291\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[0.73421425742909374,\n",
       "  0.4711306104815271,\n",
       "  0.3268119439164856,\n",
       "  0.29166804714643041,\n",
       "  0.28174034275907389,\n",
       "  0.27563732879608793,\n",
       "  0.26901115514504736,\n",
       "  0.26069326348133876,\n",
       "  0.25095498381744191,\n",
       "  0.24072354852386152,\n",
       "  0.23050569646648644,\n",
       "  0.22056711057831579,\n",
       "  0.21105138084767061,\n",
       "  0.20202323983137913,\n",
       "  0.193514199804078,\n",
       "  0.18553479854789648,\n",
       "  0.17808233035086557,\n",
       "  0.17114605447700848,\n",
       "  0.16471003287078342,\n",
       "  0.15875348675251133,\n",
       "  0.15325155443248811,\n",
       "  0.14817562802749734,\n",
       "  0.1434951164835904,\n",
       "  0.13917805194541152,\n",
       "  0.13519336567665294,\n",
       "  0.13151055102207335,\n",
       "  0.12810290632961693,\n",
       "  0.12494674835933539,\n",
       "  0.12203270484528893,\n",
       "  0.11938485443338086,\n",
       "  0.1171484004874486,\n",
       "  0.11598200927565804,\n",
       "  0.11772074094146054,\n",
       "  0.13295632771896415,\n",
       "  0.16190722444536748,\n",
       "  0.29307029372104926,\n",
       "  0.11794671896058945,\n",
       "  0.10983450446498617,\n",
       "  0.10750552948054111,\n",
       "  0.10742232153238501,\n",
       "  0.1132624222271401,\n",
       "  0.11651706920434014,\n",
       "  0.14354744316097617,\n",
       "  0.12881621499193532,\n",
       "  0.16963096486733967,\n",
       "  0.11424465719682433,\n",
       "  0.12263761978419847,\n",
       "  0.10644398806751171,\n",
       "  0.11199735793921586,\n",
       "  0.10096809547860501,\n",
       "  0.10405525334942206,\n",
       "  0.096185318959555316,\n",
       "  0.097757904446822308,\n",
       "  0.092056193709779452,\n",
       "  0.092725389036751371,\n",
       "  0.088512021411133429,\n",
       "  0.088657256654662106,\n",
       "  0.085455743946453513,\n",
       "  0.085300465320333738,\n",
       "  0.082789565670635445,\n",
       "  0.082461926485496176,\n",
       "  0.080430273002618818,\n",
       "  0.080003210027086283,\n",
       "  0.078312238006528312,\n",
       "  0.07782741901877363,\n",
       "  0.076385372242174149,\n",
       "  0.075866946461702861,\n",
       "  0.074612010561897271,\n",
       "  0.074074388794211812,\n",
       "  0.07296404465508953,\n",
       "  0.072416259455206994,\n",
       "  0.071420568636488171,\n",
       "  0.070868683646369196,\n",
       "  0.069966012959432639,\n",
       "  0.069414421428916712,\n",
       "  0.068588693338400439,\n",
       "  0.068040788166343832,\n",
       "  0.067279702555191195,\n",
       "  0.066738198741101526,\n",
       "  0.066032082425918362,\n",
       "  0.065499158875021182,\n",
       "  0.064840222904191849,\n",
       "  0.064317583320824642,\n",
       "  0.063699442832603773,\n",
       "  0.063188352545769766,\n",
       "  0.062605711775312323,\n",
       "  0.062107038216388445,\n",
       "  0.061555475909191741,\n",
       "  0.061069741068684839,\n",
       "  0.060545554897250227,\n",
       "  0.060072996847966444,\n",
       "  0.059573082113738546,\n",
       "  0.059113718170832233,\n",
       "  0.058635467463178666,\n",
       "  0.058189151774515321,\n",
       "  0.057730369365707551,\n",
       "  0.057296840440129337,\n",
       "  0.056855669034657524,\n",
       "  0.056434585968367174,\n",
       "  0.056009445017330482,\n",
       "  0.055600413699401004,\n",
       "  0.055189948754869107,\n",
       "  0.054792540611110795,\n",
       "  0.054395582853738612,\n",
       "  0.054009348809305829,\n",
       "  0.053624883422922653,\n",
       "  0.053249365140036295,\n",
       "  0.052876506895864907,\n",
       "  0.052511246453216698,\n",
       "  0.05214922079542797,\n",
       "  0.051793769190204855,\n",
       "  0.051441897261226603,\n",
       "  0.051095821621852097,\n",
       "  0.050753507952698153,\n",
       "  0.050416397172954097,\n",
       "  0.05008311910409597,\n",
       "  0.049754587666784825,\n",
       "  0.049429885890149516,\n",
       "  0.049109575826422974,\n",
       "  0.048793045708113006,\n",
       "  0.048480626837480903,\n",
       "  0.048171910378047376,\n",
       "  0.047867079130675248,\n",
       "  0.047565857546555027,\n",
       "  0.047268334757095812,\n",
       "  0.046974321733390903,\n",
       "  0.046683849814044054,\n",
       "  0.046396785498741017,\n",
       "  0.046113125362874652,\n",
       "  0.045832771159891553,\n",
       "  0.04555569919621253,\n",
       "  0.045281833382547924,\n",
       "  0.045011138692930934,\n",
       "  0.044743552845385733,\n",
       "  0.04447903487308319,\n",
       "  0.044217531051981403,\n",
       "  0.043958997652189871,\n",
       "  0.043703386259743543,\n",
       "  0.04345065220746662,\n",
       "  0.043200750420180449,\n",
       "  0.042953636312581776,\n",
       "  0.042709266980695527,\n",
       "  0.042467598470930679,\n",
       "  0.042228589381520569,\n",
       "  0.04199219667469227,\n",
       "  0.041758380085734972,\n",
       "  0.041527097630804213,\n",
       "  0.041298309998087125,\n",
       "  0.041071976318266291,\n",
       "  0.040848058152796726,\n",
       "  0.040626515767957283,\n",
       "  0.040407311576529374,\n",
       "  0.040190406982279457,\n",
       "  0.039975765256951269,\n",
       "  0.039763348934994225,\n",
       "  0.039553122167917287,\n",
       "  0.039345048610510747,\n",
       "  0.039139093318819401,\n",
       "  0.038935221056507284,\n",
       "  0.038733397808065131,\n",
       "  0.038533589434512718,\n",
       "  0.038335762869600179,\n",
       "  0.038139885060605012,\n",
       "  0.037945923907507333,\n",
       "  0.037753847433392371,\n",
       "  0.037563624517666185,\n",
       "  0.03737522424956443,\n",
       "  0.037188616497814422,\n",
       "  0.037003771409056731,\n",
       "  0.036820659848568325,\n",
       "  0.036639253012662555,\n",
       "  0.036459522768375989,\n",
       "  0.036281441355046996,\n",
       "  0.036104981645239606,\n",
       "  0.035930116915773233,\n",
       "  0.035756821047519184,\n",
       "  0.035585068350283029,\n",
       "  0.035414833715337386,\n",
       "  0.035246092481879411,\n",
       "  0.035078820553124508,\n",
       "  0.034912994294708323,\n",
       "  0.034748590622735415,\n",
       "  0.034585586926588294,\n",
       "  0.034423961135393803,\n",
       "  0.034263691659346313,\n",
       "  0.034104757439529655,\n",
       "  0.033947137903147483,\n",
       "  0.033790813000442038,\n",
       "  0.033635763170227824,\n",
       "  0.033481969366717261,\n",
       "  0.033329413032536288,\n",
       "  0.033178076117550456,\n",
       "  0.033027941057154661,\n",
       "  0.032878990784649904,\n",
       "  0.032731208713085665,\n",
       "  0.032584578742335411,\n",
       "  0.032439085243161712,\n",
       "  0.032294713059850634,\n",
       "  0.032151447495492126,\n",
       "  0.032009274310837409,\n",
       "  0.031868179710058883,\n",
       "  0.031728150336371434,\n",
       "  0.031589173257769228,\n",
       "  0.031451235959884132,\n",
       "  0.03131432633139896,\n",
       "  0.031178432654588105,\n",
       "  0.031043543590280817,\n",
       "  0.030909648166525982,\n",
       "  0.030776735763125593,\n",
       "  0.03064479609887415,\n",
       "  0.030513819215799657,\n",
       "  0.030383795465437069,\n",
       "  0.030254715493000735,\n",
       "  0.030126570223151499,\n",
       "  0.029999350844378766,\n",
       "  0.029873048794706852,\n",
       "  0.029747655746587342,\n",
       "  0.029623163592958308,\n",
       "  0.029499564432941078,\n",
       "  0.029376850558616302,\n",
       "  0.029255014441787096,\n",
       "  0.029134048721771378,\n",
       "  0.029013946193441643,\n",
       "  0.028894699796257584,\n",
       "  0.028776302603727794,\n",
       "  0.0286587478138258,\n",
       "  0.028542028739948062,\n",
       "  0.028426138802775489,\n",
       "  0.028311071522730945,\n",
       "  0.028196820513272811,\n",
       "  0.028083379474789213,\n",
       "  0.027970742189243822,\n",
       "  0.027858902515387605,\n",
       "  0.027747854384622864,\n",
       "  0.027637591797368959,\n",
       "  0.027528108819970448,\n",
       "  0.027419399582022379,\n",
       "  0.027311458274122617,\n",
       "  0.0272042791459455,\n",
       "  0.027097856504626879,\n",
       "  0.026992184713371111,\n",
       "  0.0268872581902589,\n",
       "  0.026783071407180926,\n",
       "  0.026679618888871416,\n",
       "  0.026576895211980086,\n",
       "  0.026474895004156802,\n",
       "  0.026373612943100064,\n",
       "  0.026273043755547513,\n",
       "  0.026173182216171809,\n",
       "  0.026074023146365893,\n",
       "  0.025975561412893151,\n",
       "  0.025877791926393251,\n",
       "  0.025780709639730608,\n",
       "  0.025684309546183708,\n",
       "  0.025588586677472231,\n",
       "  0.025493536101627697,\n",
       "  0.025399152920713201,\n",
       "  0.025305432268404471,\n",
       "  0.025212369307444926,\n",
       "  0.025119959226992297,\n",
       "  0.025028197239874733,\n",
       "  0.024937078579777727,\n",
       "  0.024846598498383235,\n",
       "  0.024756752262484392,\n",
       "  0.024667535151098822,\n",
       "  0.024578942452604256,\n",
       "  0.024490969461919546,\n",
       "  0.024403611477753447,\n",
       "  0.024316863799942728,\n",
       "  0.024230721726899721,\n",
       "  0.024145180553187789,\n",
       "  0.024060235567241704,\n",
       "  0.023975882049247579,\n",
       "  0.023892115269195411,\n",
       "  0.023808930485114781,\n",
       "  0.023726322941502398,\n",
       "  0.023644287867947724,\n",
       "  0.023562820477961318,\n",
       "  0.023481915968007753,\n",
       "  0.023401569516743988,\n",
       "  0.023321776284461387,\n",
       "  0.023242531412728584,\n",
       "  0.023163830024230701,\n",
       "  0.02308566722279904,\n",
       "  0.023008038093624401,\n",
       "  0.022930937703646265,\n",
       "  0.022854361102109186,\n",
       "  0.022778303321277331,\n",
       "  0.022702759377297741,\n",
       "  0.022627724271202358,\n",
       "  0.0225531929900393,\n",
       "  0.022479160508123375,\n",
       "  0.022405621788396512,\n",
       "  0.022332571783888647,\n",
       "  0.022260005439270456,\n",
       "  0.022187917692489331,\n",
       "  0.02211630347648089,\n",
       "  0.022045157720948848,\n",
       "  0.021974475354206546,\n",
       "  0.02190425130507425,\n",
       "  0.021834480504826954,\n",
       "  0.021765157889188137,\n",
       "  0.021696278400365424,\n",
       "  0.021627836989125009,\n",
       "  0.021559828616901949,\n",
       "  0.021492248257944255,\n",
       "  0.021425090901489235,\n",
       "  0.021358351553970655,\n",
       "  0.02129202524125607,\n",
       "  0.02122610701091374,\n",
       "  0.021160591934508782,\n",
       "  0.021095475109928423,\n",
       "  0.021030751663736219,\n",
       "  0.02096641675355506,\n",
       "  0.020902465570478625,\n",
       "  0.02083889334151073,\n",
       "  0.020775695332031604,\n",
       "  0.020712866848289761,\n",
       "  0.020650403239917559,\n",
       "  0.02058829990246774,\n",
       "  0.02052655227996775,\n",
       "  0.020465155867487657,\n",
       "  0.020404106213716484,\n",
       "  0.02034339892354102,\n",
       "  0.020283029660619928,\n",
       "  0.020222994149944884,\n",
       "  0.02016328818037957,\n",
       "  0.020103907607165961,\n",
       "  0.020044848354386333,\n",
       "  0.019986106417368497,\n",
       "  0.019927677865020414,\n",
       "  0.019869558842079794,\n",
       "  0.01981174557126324,\n",
       "  0.019754234355298984,\n",
       "  0.019697021578826617,\n",
       "  0.019640103710147204,\n",
       "  0.019583477302806893,\n",
       "  0.019527138996997574,\n",
       "  0.019471085520758342,\n",
       "  0.019415313690962704,\n",
       "  0.019359820414077189,\n",
       "  0.019304602686678645,\n",
       "  0.019249657595719099,\n",
       "  0.019194982318529081,\n",
       "  0.019140574122552535,\n",
       "  0.019086430364808938,\n",
       "  0.019032548491080922,\n",
       "  0.018978926034828623,\n",
       "  0.018925560615834745,\n",
       "  0.018872449938587652,\n",
       "  0.01881959179041242,\n",
       "  0.018766984039362989,\n",
       "  0.018714624631891316,\n",
       "  0.018662511590311778,\n",
       "  0.018610643010081748,\n",
       "  0.018559017056920986,\n",
       "  0.01850763196379436,\n",
       "  0.018456486027783478,\n",
       "  0.018405577606873797,\n",
       "  0.018354905116684039,\n",
       "  0.018304467027164772,\n",
       "  0.018254261859292475,\n",
       "  0.018204288181784368,\n",
       "  0.018154544607858245,\n",
       "  0.018105029792059493,\n",
       "  0.018055742427175833,\n",
       "  0.018006681241257892,\n",
       "  0.017957844994761284,\n",
       "  0.017909232477823427,\n",
       "  0.017860842507685566,\n",
       "  0.017812673926267845,\n",
       "  0.017764725597902688,\n",
       "  0.017716996407229166,\n",
       "  0.017669485257248797,\n",
       "  0.017622191067540753,\n",
       "  0.017575112772632819,\n",
       "  0.017528249320522354,\n",
       "  0.017481599671340258,\n",
       "  0.017435162796149577,\n",
       "  0.017388937675869433,\n",
       "  0.017342923300314221,\n",
       "  0.017297118667337623,\n",
       "  0.017251522782070559,\n",
       "  0.017206134656242329,\n",
       "  0.017160953307574313,\n",
       "  0.017115977759235665,\n",
       "  0.017071207039351402,\n",
       "  0.017026640180553179,\n",
       "  0.016982276219564277,\n",
       "  0.01693811419681068,\n",
       "  0.016894153156051018,\n",
       "  0.016850392144018917,\n",
       "  0.016806830210072059,\n",
       "  0.016763466405843051,\n",
       "  0.016720299784887961,\n",
       "  0.016677329402329067,\n",
       "  0.016634554314489142,\n",
       "  0.016591973578515072,\n",
       "  0.01654958625198941,\n",
       "  0.01650739139252877,\n",
       "  0.016465388057368642,\n",
       "  0.016423575302934472,\n",
       "  0.016381952184399327,\n",
       "  0.016340517755228802,\n",
       "  0.016299271066713934,\n",
       "  0.016258211167493281,\n",
       "  0.01621733710306553,\n",
       "  0.016176647915293919,\n",
       "  0.016136142641904105,\n",
       "  0.016095820315977154,\n",
       "  0.016055679965439183,\n",
       "  0.016015720612549557,\n",
       "  0.015975941273389183,\n",
       "  0.015936340957350789,\n",
       "  0.01589691866663279,\n",
       "  0.01585767339573841,\n",
       "  0.015818604130981767,\n",
       "  0.015779709850002408,\n",
       "  0.015740989521289862,\n",
       "  0.015702442103719707,\n",
       "  0.015664066546102438,\n",
       "  0.01562586178674661,\n",
       "  0.015587826753037387,\n",
       "  0.015549960361031748,\n",
       "  0.015512261515071455,\n",
       "  0.015474729107414861,\n",
       "  0.015437362017888459,\n",
       "  0.015400159113559193,\n",
       "  0.015363119248428274,\n",
       "  0.015326241263147361,\n",
       "  0.01528952398475782,\n",
       "  0.01525296622645372,\n",
       "  0.015216566787369218,\n",
       "  0.015180324452390868,\n",
       "  0.015144237991995522,\n",
       "  0.015108306162114131,\n",
       "  0.015072527704022185,\n",
       "  0.015036901344257034,\n",
       "  0.015001425794562744,\n",
       "  0.014966099751862819,\n",
       "  0.01493092189826134,\n",
       "  0.014895890901073006,\n",
       "  0.014861005412882567,\n",
       "  0.014826264071634267,\n",
       "  0.014791665500751901,\n",
       "  0.014757208309290197,\n",
       "  0.01472289109211824,\n",
       "  0.014688712430135864,\n",
       "  0.014654670890524007,\n",
       "  0.014620765027030032,\n",
       "  0.014586993380289378,\n",
       "  0.014553354478184817,\n",
       "  0.014519846836245003,\n",
       "  0.014486468958083951,\n",
       "  0.014453219335883512,\n",
       "  0.014420096450920931,\n",
       "  0.014387098774143969,\n",
       "  0.014354224766796212,\n",
       "  0.014321472881095514,\n",
       "  0.01428884156096881,\n",
       "  0.014256329242846717,\n",
       "  0.014223934356521875,\n",
       "  0.014191655326074899,\n",
       "  0.014159490570872662,\n",
       "  0.014127438506643358,\n",
       "  0.014095497546633582,\n",
       "  0.014063666102852607,\n",
       "  0.01403194258740962,\n",
       "  0.01400032541394962,\n",
       "  0.013968812999194197,\n",
       "  0.013937403764593355,\n",
       "  0.013906096138094792,\n",
       "  0.013874888556037026,\n",
       "  0.013843779465172871,\n",
       "  0.013812767324829299,\n",
       "  0.013781850609209953,\n",
       "  0.013751027809845754,\n",
       "  0.013720297438198763,\n",
       "  0.013689658028423679,\n",
       "  0.01365910814029047,\n",
       "  0.013628646362270406,\n",
       "  0.013598271314786581,\n",
       "  0.013567981653628339,\n",
       "  0.013537776073526981,\n",
       "  0.01350765331188826,\n",
       "  0.013477612152674453,\n",
       "  0.013447651430426196,\n",
       "  0.013417770034411079,\n",
       "  0.013387966912882873,\n",
       "  0.013358241077431306,\n",
       "  0.013328591607398825,\n",
       "  0.013299017654336403,\n",
       "  0.013269518446466474,\n",
       "  0.013240093293116598,\n",
       "  0.013210741589083329,\n",
       "  0.013181462818881455,\n",
       "  0.013152256560829842,\n",
       "  0.013123122490921412,\n",
       "  0.013094060386421556,\n",
       "  0.013065070129136762,\n",
       "  0.013036151708293262]]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signet.train(images, labels, iters, alpha, silent=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "toBinary = np.vectorize(lambda x: 1 if x >= .5 else 0)\n",
    "compare = np.vectorize(lambda a, b: 1 if a==b else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sigPred = toBinary(signet.predict(tImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = toBinary(net.predict(tImages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 1, 1, ..., 1, 0, 1]])"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compare(sigPred, pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "sigPred[0][-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98109999999999997"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "signet.test(tImages, tLabels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
